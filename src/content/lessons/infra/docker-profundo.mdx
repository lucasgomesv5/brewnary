---
title: "Docker Profundo"
description: "Arquitetura interna do Docker Engine, otimização de builds com BuildKit, storage drivers, networking avançado e segurança de containers"
track: "infra"
order: 2
section: "Containers"
priority: "high"
tags: ["docker", "buildkit", "overlay2", "networking", "segurança"]
prerequisites: ["virtualizacao-containers"]
keyTakeaways:
  - "O Docker Engine é composto por dockerd → containerd → runc/shim, cada camada com responsabilidades distintas no lifecycle do container"
  - "BuildKit com cache mounts e builds multi-plataforma reduz drasticamente tempo de build e tamanho de imagem"
  - "Segurança de containers exige configuração ativa: rootless mode, seccomp profiles, capabilities drop e content trust para assinatura de imagens"
---

## Arquitetura do Docker Engine

O Docker não é um monólito — é uma composição de componentes com responsabilidades bem definidas. Entender essa arquitetura é essencial para debugging em produção.

```
docker CLI ──(REST API)──→ dockerd (daemon)
                              │
                              ├──→ containerd (gerenciamento de lifecycle)
                              │       │
                              │       ├──→ containerd-shim-runc-v2
                              │       │       │
                              │       │       └──→ runc (cria o container)
                              │       │              │
                              │       │              └──→ processo isolado
                              │       │
                              │       └──→ snapshotter (gerenciamento de layers)
                              │
                              └──→ BuildKit (construção de imagens)
```

### Papel de Cada Componente

**dockerd**: daemon que expõe a API REST (por padrão em `/var/run/docker.sock`). Orquestra builds, networking, volumes. Não executa containers diretamente.

**containerd**: runtime de alto nível (CNCF graduated). Gerencia imagens, snapshots, execução de containers. Pode funcionar independentemente do Docker — Kubernetes o utiliza diretamente.

**runc**: runtime de baixo nível (referência OCI). Cria namespaces, configura cgroups, executa `pivot_root` e inicia o processo do container. Após criar o container, o runc sai — o processo fica sob supervisão do shim.

**containerd-shim**: processo intermediário entre containerd e o processo do container. Permite que o containerd seja reiniciado sem matar containers em execução. Mantém stdin/stdout/stderr abertos e reporta o exit status.

```bash
# Ver a árvore de processos de um container em execução
pstree -p $(pgrep dockerd)
# dockerd(1234)───containerd(1235)───containerd-shim(5678)───nginx(5680)

# Ver processos do containerd diretamente
ctr --namespace moby containers list
ctr --namespace moby tasks list

# Comunicação direta com containerd (sem dockerd)
ctr images pull docker.io/library/nginx:alpine
ctr run --rm docker.io/library/nginx:alpine test-container
```

## Image Building: BuildKit Avançado

BuildKit é o builder moderno do Docker — paraleliza estágios, tem cache inteligente e suporta builds multi-plataforma.

### Cache Mounts

Cache mounts persistem diretórios entre builds, sem que o conteúdo entre na imagem final. Ideal para caches de gerenciadores de pacotes.

```dockerfile
# syntax=docker/dockerfile:1
FROM node:20-alpine AS builder

WORKDIR /app
COPY package*.json ./

# Cache mount para npm — o cache persiste entre builds
# mesmo que a layer seja invalidada
RUN --mount=type=cache,target=/root/.npm \
    npm ci --prefer-offline

COPY . .
RUN npm run build

# Para Go:
FROM golang:1.22-alpine AS builder
WORKDIR /app
COPY go.mod go.sum ./
RUN --mount=type=cache,target=/go/pkg/mod \
    --mount=type=cache,target=/root/.cache/go-build \
    go mod download
COPY . .
RUN --mount=type=cache,target=/root/.cache/go-build \
    CGO_ENABLED=0 go build -o /app/server ./cmd/server

# Para Python:
FROM python:3.12-slim AS builder
WORKDIR /app
COPY requirements.txt .
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --user -r requirements.txt
```

### Secret Mounts

Permitem usar segredos durante o build sem que fiquem persistidos em nenhuma layer.

```dockerfile
# Usar chave SSH para clonar repositório privado
RUN --mount=type=secret,id=ssh_key,target=/root/.ssh/id_rsa \
    git clone git@github.com:empresa/lib-privada.git

# No build:
docker build --secret id=ssh_key,src=$HOME/.ssh/id_rsa .
```

### Builds Multi-Plataforma

```bash
# Criar builder com suporte multi-plataforma
docker buildx create --name multiarch --driver docker-container --use
docker buildx inspect --bootstrap

# Build para múltiplas arquiteturas
docker buildx build \
  --platform linux/amd64,linux/arm64 \
  --tag registry.io/app:v1.0.0 \
  --push \
  .

# Internamente, BuildKit usa QEMU para emulação ou builders nativos
# O manifest list resultante aponta para a imagem correta por arch
docker manifest inspect registry.io/app:v1.0.0
```

## Otimização de Layers

### Ordem de Instruções e Cache

```dockerfile
# ❌ INEFICIENTE: qualquer mudança no código invalida TODO o cache
FROM node:20-alpine
WORKDIR /app
COPY . .                    # Layer invalidada a cada commit
RUN npm ci                  # Reinstala tudo, mesmo sem mudança no package.json
RUN npm run build

# ✅ OTIMIZADO: dependências cacheadas separadamente do código
FROM node:20-alpine
WORKDIR /app
COPY package.json package-lock.json ./
RUN npm ci                  # Só roda se package*.json mudar
COPY tsconfig.json ./
COPY src/ ./src/
RUN npm run build           # Roda se código mudar, mas npm ci está cacheado
```

### Multi-stage com Targets Específicos

```dockerfile
# syntax=docker/dockerfile:1
FROM node:20-alpine AS base
WORKDIR /app
COPY package*.json ./

FROM base AS deps
RUN --mount=type=cache,target=/root/.npm \
    npm ci

FROM base AS prod-deps
RUN --mount=type=cache,target=/root/.npm \
    npm ci --omit=dev

FROM deps AS build
COPY tsconfig.json ./
COPY src/ ./src/
RUN npm run build

# Stage de teste — pode ser executado isoladamente
FROM deps AS test
COPY tsconfig.json jest.config.ts ./
COPY src/ ./src/
COPY tests/ ./tests/
CMD ["npm", "test"]

# Stage de produção — mínimo necessário
FROM node:20-alpine AS production
WORKDIR /app
RUN addgroup -S app && adduser -S app -G app

COPY --from=prod-deps --chown=app:app /app/node_modules ./node_modules
COPY --from=build --chown=app:app /app/dist ./dist
COPY --from=base --chown=app:app /app/package.json ./

USER app

HEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3 \
  CMD wget -qO- http://localhost:3000/health || exit 1

EXPOSE 3000
CMD ["node", "dist/server.js"]
```

```bash
# Build apenas do stage de teste
docker build --target test -t app:test .
docker run --rm app:test

# Build de produção
docker build --target production -t app:prod .

# Analisar tamanho das layers
docker history app:prod
# Resultado típico: ~80MB vs ~400MB do stage de build
```

### Redução de Tamanho com Distroless e Scratch

```dockerfile
# Go com imagem scratch (0 bytes de base)
FROM golang:1.22-alpine AS builder
WORKDIR /app
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -ldflags="-s -w" -o /server

FROM scratch
COPY --from=builder /server /server
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
EXPOSE 8080
ENTRYPOINT ["/server"]
# Resultado: ~10-15MB

# Alternativa: distroless do Google (tem CA certs, tzdata, usuário nonroot)
FROM gcr.io/distroless/static-debian12:nonroot
COPY --from=builder /server /server
ENTRYPOINT ["/server"]
```

## Storage Drivers

O storage driver determina como layers são armazenadas e compostas no disco.

```
┌─────────────────────────────────────────────────────────────────┐
│ Driver    │ Backing FS   │ Mecanismo       │ Status            │
├───────────┼──────────────┼─────────────────┼───────────────────┤
│ overlay2  │ ext4/xfs     │ OverlayFS       │ ✅ Recomendado    │
│ btrfs     │ btrfs        │ Subvolumes/CoW  │ Nicho             │
│ zfs       │ zfs          │ Clones/snapshots│ Nicho             │
│ fuse-over │ qualquer     │ FUSE overlay    │ Rootless          │
│ vfs       │ qualquer     │ Cópia completa  │ Debug (lento)     │
└─────────────────────────────────────────────────────────────────┘
```

```bash
# Verificar storage driver em uso
docker info | grep "Storage Driver"
# Storage Driver: overlay2

# Ver uso de disco detalhado
docker system df -v

# Limpar layers não utilizadas (dangling)
docker image prune -f

# Limpar TUDO não utilizado (imagens, containers, volumes, networks)
docker system prune -a --volumes
```

### Overlay2 em Detalhe

```bash
# Estrutura no disco para overlay2
/var/lib/docker/overlay2/
├── <layer-hash>/
│   ├── diff/       # Conteúdo real desta layer
│   ├── link        # Nome curto simbólico
│   ├── lower       # Referência para layers inferiores
│   ├── merged/     # Ponto de mount unificado (apenas para container ativo)
│   └── work/       # Diretório de trabalho do overlayfs
└── l/              # Links simbólicos curtos para evitar limite de mount

# Ver o mount de um container ativo
cat /proc/$(docker inspect -f '{{.State.Pid}}' container_id)/mountinfo | grep overlay
```

## Networking Deep Dive

### Bridge (Padrão)

```bash
# Docker cria a bridge docker0 e veth pairs para cada container
ip link show docker0
bridge link show

# Cada container recebe:
# - Um veth pair (vethXXXX no host ↔ eth0 no container)
# - IP da subnet da bridge (172.17.0.x por padrão)
# - Regras iptables para NAT (MASQUERADE) no tráfego de saída
# - Port mapping via iptables DNAT para tráfego de entrada

iptables -t nat -L -n | grep DNAT
# DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.17.0.2:3000
```

### Macvlan e IPvlan

```bash
# Macvlan — container recebe MAC address próprio na rede física
docker network create -d macvlan \
  --subnet=192.168.1.0/24 \
  --gateway=192.168.1.1 \
  -o parent=eth0 \
  rede-macvlan

# Container aparece como dispositivo físico na LAN
# Útil para: aplicações legacy que precisam de IP na rede local

# IPvlan L2 — similar ao macvlan, mas compartilha MAC do host
docker network create -d ipvlan \
  --subnet=192.168.1.0/24 \
  --gateway=192.168.1.1 \
  -o parent=eth0 \
  -o ipvlan_mode=l2 \
  rede-ipvlan

# IPvlan L3 — roteamento layer 3, sem broadcast
docker network create -d ipvlan \
  --subnet=10.10.0.0/24 \
  -o parent=eth0 \
  -o ipvlan_mode=l3 \
  rede-ipvlan-l3
```

### DNS Interno e Service Discovery

```bash
# Em redes user-defined, Docker embute um DNS server em 127.0.0.11
# Containers se resolvem por nome

docker network create app-net
docker run -d --name db --network app-net postgres:16
docker run -d --name api --network app-net \
  -e DATABASE_HOST=db \   # Resolverá para o IP do container 'db'
  minha-api:latest

# Dentro do container:
cat /etc/resolv.conf
# nameserver 127.0.0.11
# options ndots:0

dig db  # Retorna o IP do container postgres

# Rede bridge default NÃO tem resolução DNS por nome
# Sempre use redes user-defined em produção
```

## Segurança do Docker

### Capabilities do Linux

```bash
# Por padrão, containers recebem um subset de capabilities:
# AUDIT_WRITE, CHOWN, DAC_OVERRIDE, FOWNER, FSETID, KILL,
# MKNOD, NET_BIND_SERVICE, NET_RAW, SETFCAP, SETGID, SETPCAP, SETUID, SYS_CHROOT

# Drop ALL e adicione apenas o necessário
docker run --cap-drop ALL --cap-add NET_BIND_SERVICE nginx

# Verificar capabilities de um container
docker inspect <container> | jq '.[0].HostConfig.CapAdd'
docker inspect <container> | jq '.[0].HostConfig.CapDrop'

# Capabilities perigosas (NUNCA em produção):
# SYS_ADMIN — equivalente a root (mount, unshare, etc.)
# SYS_PTRACE — debug de processos (container escape possível)
# NET_ADMIN — manipular rede do host
# SYS_RAWIO — acesso direto a hardware
```

### Seccomp Profiles

Seccomp filtra syscalls no nível do kernel. O Docker vem com um perfil padrão que bloqueia ~44 syscalls perigosas (como `reboot`, `mount`, `kexec_load`).

```json
// custom-seccomp.json — exemplo restritivo
{
  "defaultAction": "SCMP_ACT_ERRNO",
  "architectures": ["SCMP_ARCH_X86_64"],
  "syscalls": [
    {
      "names": ["read", "write", "open", "close", "stat", "fstat",
                "mmap", "mprotect", "munmap", "brk", "ioctl",
                "access", "pipe", "select", "sched_yield",
                "socket", "connect", "accept", "sendto", "recvfrom",
                "bind", "listen", "epoll_create", "epoll_wait",
                "epoll_ctl", "clone", "execve", "exit", "exit_group",
                "futex", "nanosleep", "getpid", "getuid"],
      "action": "SCMP_ACT_ALLOW"
    }
  ]
}
```

```bash
docker run --security-opt seccomp=custom-seccomp.json minha-app
```

### User Namespaces (Remapping)

```bash
# Configurar remapeamento de UIDs
# /etc/docker/daemon.json
{
  "userns-remap": "default"
}

# Docker cria o usuário dockremap com ranges em /etc/subuid e /etc/subgid
# Root (UID 0) no container → UID 100000 no host
# Se o container escapar, o processo é um UID sem privilégios no host
```

### Docker Content Trust (Assinatura de Imagens)

```bash
# Habilitar verificação obrigatória de assinatura
export DOCKER_CONTENT_TRUST=1

# Assinar e publicar imagem
docker trust sign registry.io/app:v1.0.0

# Ver assinaturas
docker trust inspect registry.io/app:v1.0.0

# Em produção, configure no daemon.json:
{
  "content-trust": {
    "mode": "enforced"
  }
}
```

## Docker Registry e Distribution Spec

```bash
# Registry próprio com garbage collection
docker run -d -p 5000:5000 \
  -v /data/registry:/var/lib/registry \
  -e REGISTRY_STORAGE_DELETE_ENABLED=true \
  registry:2

# Garbage collection — remove blobs não referenciados
docker exec registry bin/registry garbage-collect \
  /etc/docker/registry/config.yml \
  --delete-untagged

# Registry config avançado (/etc/docker/registry/config.yml)
version: 0.1
storage:
  s3:
    accesskey: AKIAIOSFODNN7EXAMPLE
    secretkey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
    region: us-east-1
    bucket: my-registry
  cache:
    blobdescriptor: inmemory
  delete:
    enabled: true
http:
  addr: :5000
  headers:
    X-Content-Type-Options: [nosniff]
```

## Debugging Containers em Produção

```bash
# nsenter — entrar nos namespaces de um container a partir do host
# Útil quando o container não tem shell/ferramentas de debug
PID=$(docker inspect -f '{{.State.Pid}}' container_name)
nsenter -t $PID -n -m -u -i -p -- bash

# Internamente, docker exec faz algo similar:
# 1. Chama containerd API para criar um novo processo
# 2. O processo entra nos namespaces existentes do container
# 3. Não cria novos namespaces — compartilha os do container alvo

# Debug de rede
nsenter -t $PID -n ss -tlnp       # Ver portas em escuta
nsenter -t $PID -n ip route        # Ver tabela de rotas
nsenter -t $PID -n iptables -L     # Ver regras de firewall

# Debug de filesystem
nsenter -t $PID -m ls /app/        # Ver arquivos no mount namespace

# Debug de processos
nsenter -t $PID -p -m ps aux       # Ver processos no PID namespace

# Ephemeral debug container (Docker 24+)
docker debug container_name
# Cria container temporário com ferramentas no mesmo namespace

# Copiar arquivos de/para container
docker cp container:/app/logs/error.log ./error.log

# Ver eventos do container em tempo real
docker events --filter container=app --format '{{json .}}'

# Monitorar recursos em tempo real
docker stats --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}\t{{.BlockIO}}"
```

## Performance: Overhead de Containers

```bash
# Overhead de syscalls — containers adicionam ~1-5% por causa de:
# - Seccomp filter (verifica cada syscall)
# - OverlayFS (copy-on-write em escritas)
# - Network namespaces (iptables traversal)

# Benchmark de rede (container vs host)
# Host network elimina overhead de NAT/iptables
docker run --network host -d nginx
# vs
docker run -p 80:80 -d nginx
# Diferença: ~5-15% em throughput para high-RPS workloads

# cgroup limits e OOM killer
docker run --memory 256m --memory-swap 256m app
# Se o processo exceder 256MB → OOM killed (SIGKILL)
# memory-swap = memory significa: zero swap permitido

# CPU limits e throttling
docker run --cpus 0.5 app
# CFS (Completely Fair Scheduler) dá 50ms a cada 100ms
# Se o processo tenta usar mais → throttled (não killed)

# Verificar throttling
cat /sys/fs/cgroup/docker/<id>/cpu.stat
# throttled_periods: 1523
# throttled_time: 892345678  (nanosegundos)

# Tip: em produção, monitore cpu.stat para detectar throttling excessivo
# Throttling causa latência em tail percentiles (p99)
```

## Checklist de Produção

```bash
# 1. Imagem
- [ ] Multi-stage build, imagem final < 100MB
- [ ] Base image sem vulnerabilidades (trivy scan)
- [ ] Sem segredos em qualquer layer (docker history --no-trunc)
- [ ] Tag específica, nunca :latest em produção

# 2. Runtime
- [ ] Usuário não-root (USER directive)
- [ ] Read-only root filesystem (--read-only)
- [ ] Capabilities mínimas (--cap-drop ALL + add específico)
- [ ] Limites de recursos (--memory, --cpus, --pids-limit)
- [ ] Healthcheck configurado

# 3. Rede
- [ ] Redes user-defined (nunca bridge default)
- [ ] Sem exposição desnecessária de portas
- [ ] TLS para comunicação entre serviços

# 4. Storage
- [ ] Volumes para dados persistentes
- [ ] tmpfs para dados temporários/sensíveis
- [ ] Log driver configurado (não stdout em disco)
```

Dominar a arquitetura interna do Docker permite diagnosticar problemas que a abstração esconde — desde builds lentos por invalidação de cache até containers mortos por OOM killer, passando por latência causada por CPU throttling em cgroups.
