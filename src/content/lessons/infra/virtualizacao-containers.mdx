---
title: "Virtualização e Containers"
description: "Da virtualização por hypervisor à containerização com OCI: namespaces, cgroups v2, overlay filesystems e runtimes modernos"
track: "infra"
order: 1
section: "Containers"
priority: "high"
tags: ["virtualização", "containers", "linux", "namespaces", "cgroups"]
prerequisites: []
keyTakeaways:
  - "Containers não são VMs — são processos Linux isolados usando namespaces e cgroups, compartilhando o kernel do host"
  - "Namespaces isolam o que o processo enxerga (PID, rede, filesystem, usuários) e cgroups v2 limitam recursos (CPU, memória, I/O)"
  - "A especificação OCI padroniza imagens e runtimes, permitindo que containerd, CRI-O, gVisor e Kata sejam intercambiáveis"
---

## Virtualização: Hypervisors Tipo 1 e Tipo 2

Antes de entender containers, é fundamental compreender a evolução da virtualização. O conceito central é o **hypervisor** — a camada de software que permite executar múltiplas máquinas virtuais sobre um único hardware físico.

### Hypervisor Tipo 1 (Bare-Metal)

O hypervisor tipo 1 executa diretamente sobre o hardware, sem um sistema operacional hospedeiro intermediário. Ele controla o acesso ao hardware e aloca recursos diretamente para cada VM.

```
┌─────────────────────────────────────────────────────┐
│  ┌──────────┐  ┌──────────┐  ┌──────────┐          │
│  │  App A   │  │  App B   │  │  App C   │          │
│  │  Libs    │  │  Libs    │  │  Libs    │          │
│  │ Guest OS │  │ Guest OS │  │ Guest OS │          │
│  │ (Linux)  │  │(Windows) │  │ (Linux)  │          │
│  └──────────┘  └──────────┘  └──────────┘          │
│          Hypervisor Tipo 1 (bare-metal)             │
│          Ex: KVM, VMware ESXi, Xen, Hyper-V        │
│          Hardware (CPU com VT-x/AMD-V)              │
└─────────────────────────────────────────────────────┘
```

**KVM (Kernel-based Virtual Machine)** é o hypervisor tipo 1 mais utilizado em Linux. Tecnicamente, ele é um módulo do kernel que transforma o Linux em um hypervisor. Cada VM é um processo QEMU no host, com KVM fornecendo aceleração via instruções de virtualização do processador (Intel VT-x / AMD-V).

```bash
# Verificar suporte a virtualização por hardware
grep -E '(vmx|svm)' /proc/cpuinfo

# Carregar módulo KVM
modprobe kvm_intel  # ou kvm_amd

# Criar VM com QEMU/KVM
qemu-system-x86_64 \
  -enable-kvm \
  -m 4096 \
  -smp 4 \
  -drive file=disk.qcow2,format=qcow2 \
  -netdev user,id=net0 \
  -device virtio-net-pci,netdev=net0
```

**VirtIO** é uma especificação de paravirtualização que permite que o guest OS coopere com o hypervisor para I/O de alta performance. Em vez de emular hardware real (lento), VirtIO define uma interface genérica e eficiente para disco, rede e memória.

### Hypervisor Tipo 2 (Hosted)

O hypervisor tipo 2 executa como um aplicativo sobre um sistema operacional hospedeiro. Exemplos: VirtualBox, VMware Workstation, Parallels.

```
┌─────────────────────────────────────────────────────┐
│  ┌──────────┐  ┌──────────┐                         │
│  │  App A   │  │  App B   │                         │
│  │ Guest OS │  │ Guest OS │                         │
│  └──────────┘  └──────────┘                         │
│      Hypervisor Tipo 2 (VirtualBox, VMware)         │
│      Host OS (Windows, macOS, Linux)                │
│      Hardware                                       │
└─────────────────────────────────────────────────────┘
```

A diferença de performance é significativa: tipo 1 tem acesso direto ao hardware, enquanto tipo 2 passa por uma camada adicional de sistema operacional.

## Containers vs VMs: O que Realmente Muda

A diferença fundamental é o **compartilhamento de kernel**. VMs virtualizam hardware completo e executam um SO inteiro. Containers compartilham o kernel do host e isolam apenas o espaço de usuário.

```
VMs (isolamento forte):                Containers (isolamento leve):
┌──────────────────────┐               ┌──────────────────────┐
│ ┌────┐ ┌────┐ ┌────┐│               │ ┌────┐ ┌────┐ ┌────┐│
│ │App │ │App │ │App ││               │ │App │ │App │ │App ││
│ │Libs│ │Libs│ │Libs││               │ │Libs│ │Libs│ │Libs││
│ │ OS │ │ OS │ │ OS ││               │ └────┘ └────┘ └────┘│
│ └────┘ └────┘ └────┘│               │   Container Runtime  │
│     Hypervisor       │               │   Kernel do Host     │
│     Hardware         │               │   Hardware           │
└──────────────────────┘               └──────────────────────┘

Boot: ~minutos                         Boot: ~milissegundos
Tamanho: GBs                           Tamanho: MBs
Overhead: alto (SO completo)           Overhead: mínimo
Isolamento: fortíssimo (hardware)      Isolamento: processo (kernel)
Densidade: ~dezenas por host           Densidade: ~centenas por host
```

**Implicação crítica de segurança**: como containers compartilham o kernel, uma vulnerabilidade no kernel compromete todos os containers no host. VMs têm isolamento por hardware — o escape é ordens de magnitude mais difícil.

## Especificação OCI (Open Container Initiative)

A OCI padroniza dois aspectos fundamentais:

1. **Runtime Specification** (`runtime-spec`): define como executar um filesystem bundle — o lifecycle do container (create, start, kill, delete), configuração de namespaces, cgroups, mounts.

2. **Image Specification** (`image-spec`): define o formato de imagens — manifest, layers, configuração. Garante que uma imagem construída com qualquer ferramenta compatível funcione em qualquer runtime compatível.

```json
// Exemplo simplificado de config.json (OCI runtime-spec)
{
  "ociVersion": "1.0.2",
  "process": {
    "terminal": false,
    "user": { "uid": 1000, "gid": 1000 },
    "args": ["./minha-app"],
    "env": ["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin"],
    "cwd": "/app"
  },
  "root": {
    "path": "rootfs",
    "readonly": true
  },
  "linux": {
    "namespaces": [
      { "type": "pid" },
      { "type": "network" },
      { "type": "mount" },
      { "type": "ipc" },
      { "type": "uts" },
      { "type": "user" },
      { "type": "cgroup" }
    ]
  }
}
```

## Container Runtimes: containerd, CRI-O, gVisor, Kata

O ecossistema de runtimes é organizado em camadas:

```
          kubectl / Docker CLI / Podman
                    │
          ┌────────┴────────┐
          │  High-level      │
          │  containerd      │  ← Gerencia lifecycle, imagens, storage
          │  CRI-O           │  ← Alternativa focada em K8s
          └────────┬────────┘
                   │
          ┌────────┴────────┐
          │  Low-level       │
          │  runc            │  ← Referência OCI, usa namespaces/cgroups
          │  crun            │  ← Alternativa em C (mais rápido)
          │  gVisor (runsc)  │  ← Kernel em userspace (segurança)
          │  Kata Containers │  ← MicroVM por container (isolamento)
          └─────────────────┘
```

- **containerd**: runtime de alto nível mantido pela CNCF. Gerencia o lifecycle completo — pull de imagens, criação de snapshots, supervisão de containers. É o runtime padrão do Docker e do Kubernetes.

- **CRI-O**: implementação minimalista da Container Runtime Interface (CRI) do Kubernetes. Não requer Docker. Usa runc por baixo.

- **gVisor**: intercepta syscalls do container e as executa em um kernel escrito em Go (Sentry) no espaço de usuário. Reduz drasticamente a superfície de ataque do kernel do host. Tradeoff: overhead de performance nas syscalls.

- **Kata Containers**: executa cada container dentro de uma microVM leve (~130ms de boot). Combina o isolamento de hardware de VMs com a ergonomia de containers. Ideal para workloads multi-tenant.

```bash
# Verificar runtime configurado no containerd
ctr version
crictl info | jq '.config.containerd.runtimes'

# Executar container com runtime alternativo no Kubernetes
# (RuntimeClass)
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: gvisor
handler: runsc
```

## Linux Namespaces: Isolamento Detalhado

Namespaces são a primitiva do kernel Linux que fornecem isolamento. Cada namespace cria uma visão restrita de um recurso global do sistema.

### Os 7 Namespaces

```
┌──────────────────────────────────────────────────────────────┐
│ Namespace │ Isolamento                │ Flag           │
├───────────┼───────────────────────────┼────────────────┤
│ pid       │ Árvore de processos       │ CLONE_NEWPID   │
│ net       │ Interfaces de rede, rotas │ CLONE_NEWNET   │
│ mnt       │ Pontos de montagem        │ CLONE_NEWNS    │
│ user      │ UIDs/GIDs                 │ CLONE_NEWUSER  │
│ uts       │ Hostname, domainname      │ CLONE_NEWUTS   │
│ ipc       │ Filas de mensagem, semáforos│ CLONE_NEWIPC  │
│ cgroup    │ Visão da hierarquia cgroup│ CLONE_NEWCGROUP│
└──────────────────────────────────────────────────────────────┘
```

**PID Namespace**: o processo dentro do container vê a si mesmo como PID 1 (init). Ele não enxerga processos do host nem de outros containers. No host, o mesmo processo aparece com um PID diferente.

```bash
# Criar um novo PID namespace manualmente
sudo unshare --pid --fork --mount-proc bash
ps aux  # Apenas o bash aparece como PID 1

# Ver os namespaces de um processo
ls -la /proc/$$/ns/
# lrwxrwxrwx 1 root root 0 ... cgroup -> 'cgroup:[4026531835]'
# lrwxrwxrwx 1 root root 0 ... ipc -> 'ipc:[4026531839]'
# lrwxrwxrwx 1 root root 0 ... mnt -> 'mnt:[4026531840]'
# lrwxrwxrwx 1 root root 0 ... net -> 'net:[4026531992]'
# lrwxrwxrwx 1 root root 0 ... pid -> 'pid:[4026531836]'
# ...
```

**Network Namespace**: cada container recebe sua própria stack de rede — interfaces, tabelas de roteamento, regras de firewall, sockets. O Docker cria um par `veth` (virtual ethernet) — uma ponta no namespace do container, outra na bridge do host.

```bash
# Criar network namespace e configurar conectividade
ip netns add container1
ip link add veth0 type veth peer name veth1
ip link set veth1 netns container1
ip addr add 10.0.0.1/24 dev veth0
ip netns exec container1 ip addr add 10.0.0.2/24 dev veth1
ip link set veth0 up
ip netns exec container1 ip link set veth1 up
ip netns exec container1 ping 10.0.0.1
```

**Mount Namespace**: isola pontos de montagem. O container vê seu próprio filesystem raiz (rootfs) sem acesso ao filesystem do host. É a base do overlay filesystem.

**User Namespace**: mapeia UIDs dentro do container para UIDs diferentes no host. Root (UID 0) dentro do container pode ser mapeado para UID 100000 no host — **rootless containers** dependem disso.

```bash
# Exemplo: processo é root dentro do namespace mas não-root no host
unshare --user --map-root-user bash
id  # uid=0(root) gid=0(root)
# Mas no host, o processo roda com o UID original
```

**UTS Namespace**: isola hostname e NIS domain name. Cada container pode ter seu próprio hostname.

**IPC Namespace**: isola System V IPC e POSIX message queues. Processos em diferentes IPC namespaces não compartilham memória via `shmget`.

**Cgroup Namespace**: virtualiza a visão de `/proc/self/cgroup` — o container vê sua hierarquia cgroup como raiz, sem conhecer a estrutura do host.

## cgroups v2: Controle Granular de Recursos

cgroups (Control Groups) limitam, contabilizam e isolam o uso de recursos (CPU, memória, I/O) de um grupo de processos. A versão 2 é a implementação unificada e recomendada.

### Diferenças entre cgroups v1 e v2

```
cgroups v1:                           cgroups v2:
- Hierarquias múltiplas e            - Hierarquia unificada única
  independentes por controlador      - Montada em /sys/fs/cgroup
- Montadas em subdiretórios          - Modelo de delegação seguro
  separados de /sys/fs/cgroup        - Suporte nativo a PSI
- Inconsistências entre              - Interface consistente
  controladores                      - Melhor suporte a rootless
```

### Configuração Prática de Limites

```bash
# Verificar se o sistema usa cgroups v2
stat -fc %T /sys/fs/cgroup/
# cgroup2fs = v2, tmpfs = v1

# Estrutura de um cgroup v2
/sys/fs/cgroup/
├── cgroup.controllers        # Controladores disponíveis
├── cgroup.subtree_control    # Controladores habilitados para filhos
├── system.slice/
│   └── docker-<container-id>.scope/
│       ├── cpu.max            # Limite de CPU
│       ├── cpu.weight         # Peso relativo (1-10000)
│       ├── memory.max         # Limite hard de memória
│       ├── memory.high        # Limite soft (throttle)
│       ├── memory.current     # Uso atual
│       ├── io.max             # Limite de I/O por dispositivo
│       ├── pids.max           # Limite de processos
│       └── cgroup.procs       # PIDs no grupo

# Exemplo: limitar CPU a 50% de 1 core e memória a 256MB
echo "50000 100000" > /sys/fs/cgroup/mygroup/cpu.max
echo "268435456" > /sys/fs/cgroup/mygroup/memory.max

# Equivalente via Docker:
docker run --cpus="0.5" --memory="256m" --memory-swap="256m" nginx
```

### PSI (Pressure Stall Information) — Exclusivo de cgroups v2

```bash
# PSI informa quanto tempo as tarefas estão em stall esperando recursos
cat /sys/fs/cgroup/mygroup/cpu.pressure
# some avg10=0.00 avg60=0.00 avg300=0.00 total=0
# full avg10=0.00 avg60=0.00 avg300=0.00 total=0

cat /sys/fs/cgroup/mygroup/memory.pressure
# some avg10=4.25 avg60=1.80 avg300=0.50 total=123456
# ↑ 4.25% do tempo nos últimos 10s, alguma tarefa esperava memória
```

## Overlay Filesystem: Layers e Copy-on-Write

O overlay filesystem é como containers compartilham camadas de imagem eficientemente. Múltiplos containers baseados na mesma imagem compartilham as mesmas layers de leitura — apenas a camada de escrita é exclusiva.

```
Container em execução:
┌───────────────────────────────────────────┐
│  Writable Layer (thin, container-specific)│ ← upperdir
├───────────────────────────────────────────┤
│  Layer N: COPY . .  (código da app)       │
│  Layer N-1: RUN npm ci (node_modules)     │ ← lowerdir (merged)
│  Layer N-2: WORKDIR /app                  │
│  Layer 1: FROM node:20-alpine (base)      │
└───────────────────────────────────────────┘

Mecanismo Copy-on-Write (CoW):
1. Leitura: busca da camada mais alta para baixo
2. Escrita em arquivo existente: copia para upperdir, modifica lá
3. Criação: arquivo novo criado diretamente no upperdir
4. Deleção: whiteout file criado no upperdir para "esconder" o original
```

```bash
# Ver as camadas de uma imagem
docker image inspect nginx --format '{{json .RootFS.Layers}}' | jq

# Ver o overlay mount de um container em execução
docker inspect <container-id> --format '{{.GraphDriver.Data.MergedDir}}'

# Mount manual de overlayfs (para entender o mecanismo)
mount -t overlay overlay \
  -o lowerdir=/lower1:/lower2,upperdir=/upper,workdir=/work \
  /merged

# Cada layer é identificada por um digest SHA256
# docker pull baixa apenas as layers que ainda não existem localmente
```

### Impacto em Performance

Cada operação de escrita em arquivo existente nas lower layers causa uma cópia completa do arquivo para a upper layer. Por isso:
- Logs dentro do container geram I/O na writable layer (use volume ou driver de logging)
- Arquivos grandes que são modificados frequentemente devem estar em volumes
- `--tmpfs /tmp` evita copy-on-write para dados temporários

## Rootless Containers

Rootless containers executam o daemon e os containers inteiros sem privilégios de root. Toda a stack roda como usuário não privilegiado.

```bash
# Instalação rootless do Docker
dockerd-rootless-setuptool.sh install

# Ou com Podman (rootless por padrão)
podman run --rm -it alpine sh

# Componentes que habilitam rootless:
# 1. User namespaces: root no container → UID alto no host
# 2. slirp4netns / pasta: networking sem root
# 3. fuse-overlayfs: overlay filesystem sem root
# 4. cgroups v2 com delegação: limites sem root

# Verificar mapeamento de UIDs
cat /etc/subuid
# usuario:100000:65536
# → UID 0 no container = UID 100000 no host
# → UID 1 no container = UID 100001 no host
```

### Implicações de Segurança

```
┌─────────────────────────────────────────────────────────────┐
│ Vetor de Ataque          │ Container Root │ Rootless        │
├──────────────────────────┼────────────────┼─────────────────┤
│ Container escape         │ root no host   │ usuário no host │
│ Kernel exploit           │ root no host   │ usuário no host │
│ Mount do Docker socket   │ acesso total   │ não aplicável   │
│ Bind em porta < 1024     │ permitido      │ não permitido   │
│ Modificar /etc no host   │ possível se    │ impossível      │
│                          │ mal configurado│                 │
└─────────────────────────────────────────────────────────────┘

Recomendação para produção:
1. Use rootless containers sempre que possível
2. Configure seccomp profiles para filtrar syscalls
3. Drop ALL capabilities, adicione apenas as necessárias
4. Use read-only root filesystem
5. Nunca monte o Docker socket dentro de containers
```

```bash
# Exemplo de security hardening máximo
docker run \
  --read-only \
  --tmpfs /tmp \
  --cap-drop ALL \
  --cap-add NET_BIND_SERVICE \
  --security-opt no-new-privileges \
  --security-opt seccomp=profile.json \
  --user 1000:1000 \
  --pids-limit 100 \
  --memory 256m \
  --cpus 0.5 \
  minha-app:latest
```

## Construindo um Container Manualmente (Sem Docker)

Para consolidar o entendimento, este é o processo que o runc executa por baixo:

```bash
#!/bin/bash
# Mini-container usando primitivas do Linux diretamente

# 1. Criar cgroup para limitar recursos
mkdir /sys/fs/cgroup/meu-container
echo "50000 100000" > /sys/fs/cgroup/meu-container/cpu.max
echo "268435456" > /sys/fs/cgroup/meu-container/memory.max
echo "50" > /sys/fs/cgroup/meu-container/pids.max

# 2. Criar rootfs (ou usar de uma imagem OCI extraída)
mkdir -p rootfs
tar -xf alpine-rootfs.tar -C rootfs/

# 3. Executar processo com namespaces isolados
unshare \
  --pid --fork \
  --mount \
  --net \
  --uts \
  --ipc \
  --cgroup \
  --user --map-root-user \
  bash -c '
    # 4. Configurar mount namespace
    mount -t proc proc rootfs/proc
    mount -t sysfs sysfs rootfs/sys
    mount -t tmpfs tmpfs rootfs/tmp

    # 5. Pivot root (trocar filesystem raiz)
    cd rootfs
    mkdir old_root
    pivot_root . old_root
    umount -l /old_root
    rmdir /old_root

    # 6. Configurar hostname
    hostname meu-container

    # 7. Executar aplicação
    exec /bin/sh
  '
```

Esse é, em essência, o que `runc create` e `runc start` fazem — com muito mais tratamento de erros, configuração via JSON e integração com o lifecycle OCI.

## Resumo das Primitivas

```
┌─────────────────────────────────────────────────────────────────┐
│ Primitiva        │ Função                  │ Usado Para         │
├──────────────────┼─────────────────────────┼────────────────────┤
│ Namespaces       │ Isolamento de visão     │ PID, rede, fs,     │
│                  │                         │ users, hostname    │
│ cgroups v2       │ Limite de recursos      │ CPU, memória, I/O, │
│                  │                         │ PIDs               │
│ OverlayFS        │ Filesystem em camadas   │ Imagens, CoW       │
│ Capabilities     │ Permissões granulares   │ Segurança          │
│ Seccomp          │ Filtro de syscalls      │ Segurança          │
│ AppArmor/SELinux │ MAC (Mandatory Access)  │ Segurança          │
│ pivot_root       │ Troca de root filesystem│ Isolamento de fs   │
└─────────────────────────────────────────────────────────────────┘
```

A compreensão profunda dessas primitivas é o que diferencia um engenheiro que "usa Docker" de um que entende por que containers funcionam, onde estão os limites de isolamento e como diagnosticar problemas em produção quando a abstração falha.
