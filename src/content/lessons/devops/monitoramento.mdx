---
title: "Monitoramento e Observabilidade"
description: "Os três pilares da observabilidade: métricas, logs e traces. Prometheus, Grafana, OpenTelemetry, alerting com SLI/SLO/SLA e metodologias USE/RED/Golden Signals"
track: "devops"
order: 6
section: "Cloud e Operações"
priority: "high"
tags: ["monitoramento", "observabilidade", "logs", "metricas", "health-check"]
prerequisites: []
keyTakeaways:
  - "Observabilidade é a capacidade de inferir o estado interno de um sistema a partir dos seus outputs — métricas, logs e traces são complementares, não alternativos"
  - "SLIs medem a experiência do usuário, SLOs definem metas internas, SLAs são compromissos contratuais — error budgets equilibram velocidade e confiabilidade"
  - "Alertas devem ser acionáveis e baseados em sintomas (SLO burn rate), não em causas — alerte sobre o que afeta o usuário, não sobre uso de CPU"
---

## Observabilidade vs Monitoramento

```
MONITORAMENTO — "O sistema está funcionando?"
  Dashboards, health checks, alarmes predefinidos.
  Responde perguntas que você JÁ sabe que precisa fazer.
  Exemplo: CPU > 90%? Disco > 80%? Aplicação respondendo?

OBSERVABILIDADE — "POR QUE o sistema está se comportando assim?"
  Capacidade de investigar problemas NOVOS e IMPREVISTOS.
  Responde perguntas que você NÃO sabia que precisaria fazer.
  Exemplo: "Por que requests do Brasil estão 3x mais lentas
  apenas para usuários com mais de 100 pedidos às terças?"

  Observabilidade = Métricas + Logs + Traces + capacidade de correlacionar

OS TRÊS PILARES:

┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│  MÉTRICAS   │  │    LOGS     │  │   TRACES    │
│             │  │             │  │             │
│ Quanto?     │  │ O quê?      │  │ Onde?       │
│ Numérico    │  │ Textual     │  │ Distribuído │
│ Agregável   │  │ Individual  │  │ Causal      │
│ Tendências  │  │ Detalhes    │  │ Fluxo       │
│             │  │             │  │             │
│ Prometheus  │  │ Loki/ELK    │  │ Jaeger      │
│ Grafana     │  │ Fluentd     │  │ OpenTelemetry│
└─────────────┘  └─────────────┘  └─────────────┘
```

## Métricas: Tipos e Instrumentação

```
TIPOS DE MÉTRICAS NO PROMETHEUS:

1. COUNTER — valor que só cresce (ou reseta para zero)
   Uso: total de requests, total de erros, bytes processados
   Exemplo: http_requests_total{method="GET", status="200"} = 150432
   Para calcular rate: rate(http_requests_total[5m])

2. GAUGE — valor que sobe e desce
   Uso: temperatura, uso de memória, número de goroutines, queue size
   Exemplo: node_memory_MemAvailable_bytes = 4294967296
   Pode usar min(), max(), avg()

3. HISTOGRAM — distribui valores em buckets predefinidos
   Uso: latência de requests, tamanho de payloads
   Cria automaticamente: _bucket, _count, _sum
   Exemplo:
     http_request_duration_seconds_bucket{le="0.1"} = 8000
     http_request_duration_seconds_bucket{le="0.25"} = 9500
     http_request_duration_seconds_bucket{le="0.5"} = 9900
     http_request_duration_seconds_bucket{le="1.0"} = 9990
     http_request_duration_seconds_bucket{le="+Inf"} = 10000
   Calcular percentis: histogram_quantile(0.99, rate(...[5m]))

4. SUMMARY — calcula percentis no cliente (menos flexível que histogram)
   Uso: quando você precisa de percentis exatos e não de agregação
   Desvantagem: não é agregável entre instâncias
   Preferir HISTOGRAM na maioria dos casos
```

```javascript
// Instrumentação com prom-client (Node.js):
const client = require('prom-client');

// Counter — total de requests HTTP:
const httpRequestsTotal = new client.Counter({
  name: 'http_requests_total',
  help: 'Total de requisições HTTP recebidas',
  labelNames: ['method', 'route', 'status_code'],
});

// Histogram — latência de requests:
const httpRequestDuration = new client.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duração das requisições HTTP em segundos',
  labelNames: ['method', 'route', 'status_code'],
  buckets: [0.01, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10],
  // Buckets ajustados para a realidade da aplicação
});

// Gauge — conexões ativas no pool de banco:
const dbPoolConnections = new client.Gauge({
  name: 'db_pool_active_connections',
  help: 'Número de conexões ativas no pool de banco de dados',
});

// Middleware Express para coletar métricas automaticamente:
app.use((req, res, next) => {
  const end = httpRequestDuration.startTimer();
  res.on('finish', () => {
    const labels = {
      method: req.method,
      route: req.route?.path || req.path,
      status_code: res.statusCode,
    };
    httpRequestsTotal.inc(labels);
    end(labels);
  });
  next();
});

// Endpoint /metrics para o Prometheus scrape:
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', client.register.contentType);
  res.end(await client.register.metrics());
});
```

## Prometheus: PromQL e Configuração

```yaml
# prometheus.yml — configuração do Prometheus:
global:
  scrape_interval: 15s       # Frequência de coleta (padrão)
  evaluation_interval: 15s   # Frequência de avaliação de regras
  scrape_timeout: 10s

# Descoberta de targets:
scrape_configs:
  - job_name: 'api'
    metrics_path: '/metrics'
    scrape_interval: 5s  # Override para este job
    static_configs:
      - targets: ['api:3000']
        labels:
          environment: 'production'

  # Service discovery para Kubernetes:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)

  # Service discovery para Docker:
  - job_name: 'docker'
    dockerswarm_sd_configs:
      - host: unix:///var/run/docker.sock
        role: tasks
```

```promql
# PromQL — Linguagem de consulta do Prometheus:

# Taxa de requests por segundo (últimos 5 minutos):
rate(http_requests_total[5m])

# Taxa de erros (5xx) como percentual:
sum(rate(http_requests_total{status_code=~"5.."}[5m]))
/
sum(rate(http_requests_total[5m]))
* 100

# Latência p99 (percentil 99):
histogram_quantile(0.99,
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
)

# Latência p99 por rota:
histogram_quantile(0.99,
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le, route)
)

# Uso de memória como percentual:
(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)
/ node_memory_MemTotal_bytes * 100

# Top 5 endpoints mais lentos:
topk(5,
  histogram_quantile(0.95,
    sum(rate(http_request_duration_seconds_bucket[5m])) by (le, route)
  )
)

# Predição: quando o disco vai encher (extrapolação linear):
predict_linear(node_filesystem_avail_bytes[6h], 24*3600) < 0
# Retorna true se o disco vai encher nas próximas 24 horas

# Aumento de erros comparado com a última semana:
rate(http_requests_total{status_code=~"5.."}[1h])
/ rate(http_requests_total{status_code=~"5.."}[1h] offset 1w)
# > 2 significa que erros dobraram em relação à semana passada
```

```yaml
# RECORDING RULES — pré-calcular queries pesadas:
# prometheus-rules.yml
groups:
  - name: api_rules
    interval: 30s
    rules:
      - record: api:http_request_rate5m
        expr: sum(rate(http_requests_total[5m])) by (route)

      - record: api:http_error_rate5m
        expr: |
          sum(rate(http_requests_total{status_code=~"5.."}[5m]))
          / sum(rate(http_requests_total[5m]))

      - record: api:http_latency_p99_5m
        expr: |
          histogram_quantile(0.99,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
          )

# ALERTING RULES — definir alertas:
  - name: api_alerts
    rules:
      - alert: HighErrorRate
        expr: api:http_error_rate5m > 0.01
        for: 5m  # Precisa estar acima por 5 minutos (evitar flapping)
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "Taxa de erro acima de 1%"
          description: "Taxa de erro atual: {{ $value | humanizePercentage }}"
          runbook_url: "https://wiki.example.com/runbooks/high-error-rate"

      - alert: HighLatency
        expr: api:http_latency_p99_5m > 1.0
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Latência p99 acima de 1 segundo"
```

## Grafana: Dashboards e Visualização

```
BOAS PRÁTICAS DE DASHBOARDS:

1. HIERARQUIA DE DASHBOARDS:
   L0 — Overview (SLOs, golden signals, status geral)
   L1 — Por serviço (métricas detalhadas de cada serviço)
   L2 — Debug (queries específicas, flamegraphs, traces)

2. VARIÁVEIS DE TEMPLATE:
   $environment = staging | production
   $service = api | worker | scheduler
   $instance = filtrar por instância específica
   Permitem que um dashboard sirva para múltiplos ambientes/serviços

3. ANNOTATIONS:
   Marcar deploys no dashboard → correlacionar mudanças com métricas
   API do Grafana: POST /api/annotations
   {"time": 1704067200, "text": "Deploy v1.2.3", "tags": ["deploy"]}

4. ALERTING NO GRAFANA:
   Grafana Unified Alerting (Grafana 9+):
   - Multi-datasource (Prometheus, Loki, CloudWatch)
   - Notification policies (roteamento por labels)
   - Silences e mute timings
   - Contact points: Slack, PagerDuty, OpsGenie, email
```

## Logging: Estruturado e Centralizado

```javascript
// LOGGING ESTRUTURADO — fundamental para observabilidade:

// NÃO FAÇA (log desestruturado):
console.log('Erro ao processar pedido 456 do usuário 123: timeout');
// Impossível de filtrar, agregar ou correlacionar automaticamente

// FAÇA (log estruturado com Pino — logger mais rápido do Node.js):
const pino = require('pino');

const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  // Em produção: nível info ou warn (não debug!)
  formatters: {
    level: (label) => ({ level: label }),
  },
  // Redact de dados sensíveis:
  redact: ['req.headers.authorization', '*.password', '*.token'],
});

// Log com contexto estruturado:
logger.info({
  event: 'order_created',
  orderId: '456',
  userId: '123',
  amount: 99.90,
  paymentMethod: 'credit_card',
  duration_ms: 45,
  correlationId: req.headers['x-correlation-id'],
}, 'Pedido criado com sucesso');

// Output JSON (uma linha por log):
// {"level":"info","time":1704067200,"event":"order_created",
//  "orderId":"456","userId":"123","amount":99.90,"duration_ms":45,
//  "correlationId":"abc-123","msg":"Pedido criado com sucesso"}

// CORRELATION ID — rastrear uma request entre serviços:
// Middleware que propaga correlation ID:
app.use((req, res, next) => {
  req.correlationId = req.headers['x-correlation-id'] || crypto.randomUUID();
  res.setHeader('x-correlation-id', req.correlationId);
  // Child logger com correlationId em todos os logs desta request:
  req.log = logger.child({ correlationId: req.correlationId });
  next();
});

// Agora todo log dentro desta request terá o correlationId:
req.log.info({ event: 'db_query', table: 'orders', duration_ms: 12 }, 'Query executada');
// Filtrar no Grafana Loki: {app="api"} |= "abc-123"
// Retorna TODOS os logs relacionados a esta request
```

```
NÍVEIS DE LOG E QUANDO USAR:

FATAL  — aplicação vai crashar. Usado antes de process.exit(1).
ERROR  — erro que afeta o usuário. Precisa de atenção (alerta).
         Ex: falha ao processar pagamento, timeout de API externa.
WARN   — situação anormal mas recuperável. Pode virar error.
         Ex: retry necessário, cache miss inesperado, rate limiting.
INFO   — eventos significativos de negócio. O "jornal" da aplicação.
         Ex: pedido criado, usuário logou, deploy iniciado.
DEBUG  — detalhes técnicos para desenvolvimento. NUNCA em produção.
         Ex: payload de request, SQL gerado, estado de variáveis.
TRACE  — extremamente detalhado. Apenas para debugging ativo.

REGRA: Em produção, nível INFO. Se investigando um problema
específico, mudar temporariamente para DEBUG com filtro por request.

STACK DE LOGGING:
- Grafana Loki: log aggregation leve, integrado com Grafana
  Não indexa o conteúdo dos logs (como Prometheus faz com métricas)
  Indexa apenas labels — queries por regex no conteúdo
  Mais barato e simples que ELK

- ELK Stack (Elasticsearch + Logstash + Kibana):
  Full-text search nos logs. Poderoso mas CARO.
  Elasticsearch consome muita RAM e storage.
  Alternativa: EFK (Fluentd no lugar de Logstash)

- Fluent Bit / Fluentd: coletores de log (shipper)
  Coleta de containers, files, syslog → envia para Loki/ELK
```

## Distributed Tracing: OpenTelemetry

```javascript
// OpenTelemetry — padrão aberto para instrumentação:
// Substitui Jaeger client, Zipkin client, etc. com API unificada.

// Instalação:
// npm install @opentelemetry/sdk-node
// npm install @opentelemetry/auto-instrumentations-node

// tracing.js — inicializar ANTES de importar qualquer módulo:
const { NodeSDK } = require('@opentelemetry/sdk-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');

const sdk = new NodeSDK({
  traceExporter: new OTLPTraceExporter({
    url: 'http://jaeger:4318/v1/traces',  // OTLP HTTP endpoint
  }),
  instrumentations: [
    getNodeAutoInstrumentations({
      // Auto-instrumenta: HTTP, Express, pg, mysql, redis, gRPC, etc.
      '@opentelemetry/instrumentation-fs': { enabled: false },
    }),
  ],
  serviceName: 'api-orders',
});
sdk.start();

// Span manual para operações de negócio:
const { trace } = require('@opentelemetry/api');
const tracer = trace.getTracer('order-service');

async function createOrder(orderData) {
  return tracer.startActiveSpan('createOrder', async (span) => {
    try {
      span.setAttribute('order.userId', orderData.userId);
      span.setAttribute('order.items_count', orderData.items.length);

      // Child span para validação:
      const order = await tracer.startActiveSpan('validateOrder', async (childSpan) => {
        const result = await validateOrder(orderData);
        childSpan.setAttribute('validation.result', 'success');
        childSpan.end();
        return result;
      });

      // Child span para pagamento:
      await tracer.startActiveSpan('processPayment', async (childSpan) => {
        childSpan.setAttribute('payment.method', orderData.paymentMethod);
        await processPayment(order);
        childSpan.end();
      });

      span.setStatus({ code: trace.SpanStatusCode.OK });
      return order;
    } catch (error) {
      span.setStatus({ code: trace.SpanStatusCode.ERROR, message: error.message });
      span.recordException(error);
      throw error;
    } finally {
      span.end();
    }
  });
}

// TRACE CONTEXT PROPAGATION:
// W3C Trace Context (padrão): headers traceparent e tracestate
// traceparent: 00-traceId-spanId-flags
// Exemplo: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01
//
// Quando serviço A chama serviço B via HTTP, o SDK automaticamente:
// 1. Injeta traceparent no header da request
// 2. Serviço B extrai e cria child span vinculado ao trace
// Resultado: trace completo mostrando o fluxo entre todos os serviços
```

```
ANATOMIA DE UM TRACE:

Trace ID: abc123 (identifica toda a operação end-to-end)
│
├── Span: API Gateway (50ms)
│   │ service: gateway
│   │ http.method: POST
│   │ http.url: /api/orders
│   │
│   └── Span: Order Service - createOrder (45ms)
│       │ service: order-service
│       │ order.userId: 123
│       │
│       ├── Span: PostgreSQL query (5ms)
│       │   │ db.system: postgresql
│       │   │ db.statement: INSERT INTO orders...
│       │
│       ├── Span: Payment Service (30ms)
│       │   │ service: payment-service
│       │   │ payment.method: credit_card
│       │   │
│       │   ├── Span: Stripe API call (25ms)
│       │   │   │ http.url: https://api.stripe.com/v1/charges
│       │   │   │ http.status_code: 200
│       │
│       └── Span: Redis cache set (1ms)
│           │ db.system: redis
│           │ db.operation: SET

FERRAMENTAS:
- Jaeger: open-source, UI rica, integração com Kubernetes
- Tempo (Grafana): integrado com Grafana/Loki, storage em object storage
- Zipkin: mais antigo, simples, leve
- AWS X-Ray: integrado com serviços AWS
```

## Alerting: SLI, SLO, SLA e Error Budgets

```
DEFINIÇÕES:

SLI (Service Level Indicator) — métrica que mede a experiência do usuário
  Exemplos:
  - Disponibilidade: % de requests com sucesso (status != 5xx)
  - Latência: % de requests completadas em < 300ms
  - Corretude: % de responses com dados corretos
  - Freshness: % de dados atualizados nos últimos 5 minutos

SLO (Service Level Objective) — meta interna para o SLI
  Exemplos:
  - 99.9% das requests com sucesso em janela de 30 dias
  - 99% das requests com latência < 300ms
  - 99.99% dos dados atualizados em 5 minutos

SLA (Service Level Agreement) — compromisso contratual com o cliente
  Geralmente mais baixo que o SLO (margem de segurança)
  Exemplo: SLO interno = 99.95%, SLA contratual = 99.9%
  Violação de SLA → consequências financeiras/contratuais

ERROR BUDGET — quanto erro é permitido antes de violar o SLO:

  SLO = 99.9% de disponibilidade em 30 dias
  Error budget = 0.1% = 30 dias × 24h × 60min × 0.001 = 43.2 minutos

  Enquanto houver budget: priorizar features e velocidade
  Budget consumido: parar features, priorizar confiabilidade

  Isso resolve o conflito entre "mover rápido" e "não quebrar":
  - Dev quer lançar features → precisa de error budget
  - Se deploys ruins consomem budget → time para de deploy
  - Incentivo natural para investir em qualidade

BURN RATE — velocidade de consumo do error budget:
  Burn rate 1x = consome todo o budget em 30 dias (normal)
  Burn rate 10x = consome todo o budget em 3 dias (alerta!)
  Burn rate 100x = consome todo o budget em 7.2 horas (crítico!)

ALERTA BASEADO EM BURN RATE (Google SRE):
  Fast burn (2% budget em 1h): page imediato
  Slow burn (5% budget em 6h): ticket de alta prioridade
  Vantagem: menos false positives que threshold simples
```

```yaml
# Alertas baseados em SLO burn rate:
groups:
  - name: slo_alerts
    rules:
      # Fast burn — consome 2% do budget em 1 hora
      - alert: SLOFastBurn
        expr: |
          (
            1 - (sum(rate(http_requests_total{status_code!~"5.."}[1h]))
            / sum(rate(http_requests_total[1h])))
          ) > (14.4 * 0.001)
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Consumo rápido do error budget de disponibilidade"

      # Slow burn — consome 5% do budget em 6 horas
      - alert: SLOSlowBurn
        expr: |
          (
            1 - (sum(rate(http_requests_total{status_code!~"5.."}[6h]))
            / sum(rate(http_requests_total[6h])))
          ) > (6 * 0.001)
        for: 15m
        labels:
          severity: warning
```

## Metodologias de Dashboard

```
USE METHOD (Brendan Gregg) — para infraestrutura (servidores, discos, rede):
  U — Utilization: % do recurso sendo utilizado
  S — Saturation: trabalho extra em fila esperando
  E — Errors: contagem de eventos de erro

  CPU:     Utilization = cpu_usage%  | Saturation = load average | Errors = machine check
  Memória: Utilization = mem_used%   | Saturation = swap usage   | Errors = OOM kills
  Disco:   Utilization = disk_busy%  | Saturation = queue depth  | Errors = I/O errors
  Rede:    Utilization = bandwidth%  | Saturation = drops/retrans| Errors = CRC errors

RED METHOD (Tom Wilkie) — para serviços (APIs, microsserviços):
  R — Rate: requests por segundo
  E — Errors: requests com falha por segundo
  D — Duration: distribuição de latência (p50, p95, p99)

  Dashboard típico RED:
  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐
  │ Request Rate │ │  Error Rate  │ │   Duration   │
  │   150 rps    │ │   0.5%       │ │ p99: 230ms   │
  │   ▃▅▆▇▆▅▃   │ │   ▁▁▂▁▁▁▁   │ │   ▃▃▄▃▃▃▃   │
  └──────────────┘ └──────────────┘ └──────────────┘

FOUR GOLDEN SIGNALS (Google SRE) — para qualquer serviço:
  1. Latência: tempo de resposta (diferenciar sucesso vs erro)
  2. Tráfego: demanda no sistema (requests/s, sessões, transações)
  3. Erros: taxa de falha (explícitos 5xx, implícitos timeout, incorretos)
  4. Saturação: quão "cheio" o serviço está (CPU, memória, fila, conexões)

  Golden Signals ≈ RED + Saturação

ESCOLHA:
- USE para recursos de infra (hardware, rede, disco)
- RED ou Golden Signals para serviços de aplicação
- Na prática: use ambos — USE para infra no L1, RED/Golden para serviços no L0
```

## APM: Application Performance Monitoring

```
APM vai além de métricas básicas — analisa performance em nível de código:

FUNCIONALIDADES:
- Transaction tracing: tempo gasto em cada operação dentro de uma request
- Database query analysis: queries lentas, N+1, explain plans
- External service calls: latência de APIs externas
- Error tracking: stack traces, frequência, impacto
- Service maps: visualização de dependências entre serviços
- Profiling: CPU/memory profiling em produção (continuous profiling)

FERRAMENTAS:
- Datadog APM: SaaS completo (caro mas poderoso)
- New Relic: SaaS com tier gratuito generoso
- Elastic APM: open-source, integrado com ELK
- Grafana Cloud: Tempo (traces) + Pyroscope (profiling) + Loki (logs)
- SigNoz: open-source, alternativa ao Datadog

CONTINUOUS PROFILING (tendência moderna):
  Coletar profiles de CPU/memória em produção com overhead mínimo (~1-3%)
  Ferramentas: Pyroscope, Parca (Polar Signals), Datadog Profiling
  Permite responder: "Qual função consome mais CPU em produção agora?"

HEALTH CHECKS ESTRUTURADOS:
```

```javascript
// Health check completo para Kubernetes/Load Balancer:
app.get('/health', async (req, res) => {
  const checks = {};
  let healthy = true;

  // Verificar PostgreSQL:
  try {
    const start = Date.now();
    await db.query('SELECT 1');
    checks.database = { status: 'up', latency_ms: Date.now() - start };
  } catch (error) {
    checks.database = { status: 'down', error: error.message };
    healthy = false;
  }

  // Verificar Redis:
  try {
    const start = Date.now();
    await redis.ping();
    checks.cache = { status: 'up', latency_ms: Date.now() - start };
  } catch (error) {
    checks.cache = { status: 'down', error: error.message };
    healthy = false;
  }

  // Verificar espaço em disco:
  checks.disk = { status: diskUsage < 90 ? 'up' : 'degraded' };

  const status = healthy ? 200 : 503;
  res.status(status).json({
    status: healthy ? 'healthy' : 'unhealthy',
    uptime: process.uptime(),
    timestamp: new Date().toISOString(),
    checks,
  });
});

// Kubernetes probes:
// livenessProbe:  /health/live  — "o processo está vivo?"
//   Se falhar → Kubernetes reinicia o pod
//   Deve ser simples (sem checar dependências externas)
//
// readinessProbe: /health/ready — "o pod pode receber tráfego?"
//   Se falhar → removido do Service (sem receber requests)
//   Checar dependências (banco, cache)
//
// startupProbe:   /health/live  — "o pod terminou de inicializar?"
//   Usa limites mais generosos (ex: timeout 60s)
//   Depois de passar, liveness/readiness assumem
```
