---
title: "Matemática Discreta"
description: "Indução matemática, combinatória, probabilidade discreta, relações, funções e o princípio da casa dos pombos — fundamentos formais que sustentam provas de correção, análise probabilística e teoria de grafos"
track: "cs"
order: 2
section: "Fundamentos Matemáticos"
priority: "high"
tags: ["indução", "combinatória", "probabilidade", "relações", "funções", "pigeonhole", "matemática-discreta"]
prerequisites: ["logica-e-matematica", "logaritmos-e-algebra"]
keyTakeaways:
  - "Indução matemática é a ferramenta formal para provar que algoritmos estão corretos — indução forte e estrutural estendem a técnica para árvores e grafos"
  - "Combinatória (permutações, combinações, coeficiente binomial) quantifica o espaço de busca de algoritmos brute-force"
  - "Probabilidade discreta e valor esperado são essenciais para análise de algoritmos randomizados (quicksort, hashing, skip lists)"
  - "Relações de equivalência particionam conjuntos (Union-Find); ordens parciais fundamentam ordenação topológica"
  - "O Princípio da Casa dos Pombos (Pigeonhole) prova a inevitabilidade de colisões em hash tables e a existência de duplicatas"
  - "Teoria de grafos formal (grau, caminhos, ciclos, planaridade) fornece a linguagem matemática para a lição de implementação"
---

# Matemática Discreta

Matemática discreta é o ramo da matemática que lida com estruturas **finitas** e **contáveis** — exatamente o tipo de objeto que computadores manipulam. Enquanto o cálculo trata de funções contínuas e limites, a matemática discreta trata de inteiros, conjuntos finitos, grafos, árvores e sequências. Esta lição cobre os pilares formais que sustentam provas de correção de algoritmos, análise de complexidade probabilística e a teoria por trás de estruturas de dados fundamentais. Cada conceito aqui tem aplicação direta e imediata em ciência da computação.

---

## 1. Indução Matemática

Indução matemática é a técnica de prova mais importante em ciência da computação. Ela permite provar que uma propriedade P(n) vale para **todos** os números naturais (ou, mais geralmente, para todas as estruturas recursivas) verificando apenas dois passos.

### 1.1 Princípio da Indução (Indução Fraca)

Para provar que P(n) vale para todo n ≥ n₀:

1. **Base**: Prove que P(n₀) é verdadeiro.
2. **Passo indutivo**: Prove que, para qualquer k ≥ n₀, se P(k) é verdadeiro (**hipótese de indução**), então P(k+1) também é verdadeiro.

A lógica é simples: a base garante P(n₀). O passo indutivo, com k = n₀, dá P(n₀+1). Aplicando novamente com k = n₀+1, obtemos P(n₀+2), e assim indefinidamente. Formalmente, o princípio se apoia no axioma de boa ordenação dos naturais.

### 1.2 Exemplo Clássico: Soma dos Primeiros n Naturais

**Teorema**: Para todo n ≥ 1, ∑ᵢ₌₁ⁿ i = n(n+1)/2.

**Prova por indução**:

**Base** (n = 1): ∑ᵢ₌₁¹ i = 1 = 1·2/2 = 1. ✓

**Passo indutivo**: Suponha que ∑ᵢ₌₁ᵏ i = k(k+1)/2 para algum k ≥ 1 (hipótese de indução). Precisamos provar que ∑ᵢ₌₁ᵏ⁺¹ i = (k+1)(k+2)/2.

```
∑ᵢ₌₁ᵏ⁺¹ i = (∑ᵢ₌₁ᵏ i) + (k+1)
           = k(k+1)/2 + (k+1)         [pela hipótese de indução]
           = k(k+1)/2 + 2(k+1)/2
           = (k+1)(k+2)/2              ✓
```

### 1.3 Exemplo: Soma de Potências de 2

**Teorema**: Para todo n ≥ 0, ∑ᵢ₌₀ⁿ 2ⁱ = 2ⁿ⁺¹ - 1.

**Base** (n = 0): 2⁰ = 1 = 2¹ - 1 = 1. ✓

**Passo indutivo**: Suponha ∑ᵢ₌₀ᵏ 2ⁱ = 2ᵏ⁺¹ - 1.

```
∑ᵢ₌₀ᵏ⁺¹ 2ⁱ = (∑ᵢ₌₀ᵏ 2ⁱ) + 2ᵏ⁺¹
            = (2ᵏ⁺¹ - 1) + 2ᵏ⁺¹       [hipótese de indução]
            = 2 · 2ᵏ⁺¹ - 1
            = 2ᵏ⁺² - 1                  ✓
```

**Aplicação em CS**: Essa identidade explica por que uma árvore binária completa de altura h tem 2ʰ⁺¹ - 1 nós.

### 1.4 Indução Forte (Indução Completa)

Na indução forte, o passo indutivo assume que P(j) vale para **todo** j com n₀ ≤ j ≤ k (não apenas para k), e prova P(k+1). A indução forte é equivalente à fraca em poder, mas é mais conveniente quando a prova de P(k+1) depende de valores anteriores a k.

**Teorema (Fundamental da Aritmética — existência)**: Todo inteiro n ≥ 2 é primo ou pode ser escrito como produto de primos.

**Base** (n = 2): 2 é primo. ✓

**Passo indutivo**: Suponha que a propriedade vale para todo j com 2 ≤ j ≤ k. Precisamos provar para k+1.

- Se k+1 é primo, a propriedade vale trivialmente.
- Se k+1 não é primo, então existem a, b com 2 ≤ a, b ≤ k tais que k+1 = a · b. Pela hipótese de indução forte, tanto a quanto b são primos ou produtos de primos. Logo, k+1 = a · b é produto de primos. ✓

### 1.5 Indução Estrutural

A indução estrutural estende o princípio para estruturas recursivas (árvores, listas, expressões). Em vez de induzir sobre n ∈ ℕ, induzimos sobre a **estrutura** do objeto.

**Teorema**: Uma árvore binária com n nós internos tem exatamente n + 1 folhas.

**Base**: Uma árvore com 0 nós internos é uma única folha. Folhas = 0 + 1 = 1. ✓

**Passo indutivo**: Considere uma árvore T com raiz r e subárvores esquerda Tₑ e direita Tᵈ. Seja nₑ o número de nós internos de Tₑ e nᵈ o de Tᵈ. O número total de nós internos de T é n = nₑ + nᵈ + 1 (contando r).

Pela hipótese de indução:
- Tₑ tem nₑ + 1 folhas
- Tᵈ tem nᵈ + 1 folhas

Total de folhas de T = (nₑ + 1) + (nᵈ + 1) = nₑ + nᵈ + 2 = (n - 1) + 2 = n + 1. ✓

**Por que isso importa**: Essa propriedade é usada na análise de árvores de decisão (decision trees) para provar lower bounds de algoritmos de ordenação: uma árvore de decisão para ordenar n elementos precisa de pelo menos n! folhas (uma para cada permutação), logo sua altura é pelo menos log₂(n!) = Ω(n log n).

### 1.6 Aplicação: Prova de Correção de Binary Search

```typescript
/**
 * Binary Search — busca o índice de target em um array ordenado.
 * Retorna o índice se encontrado, ou -1 caso contrário.
 *
 * PROVA DE CORREÇÃO POR INDUÇÃO:
 *
 * Invariante de loop: Se target está em arr, então target está em arr[lo..hi].
 *
 * Base: Antes do loop, lo = 0, hi = arr.length - 1.
 *       arr[0..arr.length-1] é o array inteiro, então a invariante vale trivialmente.
 *
 * Passo indutivo: Suponha que a invariante vale no início da iteração.
 *   - Calculamos mid = lo + Math.floor((hi - lo) / 2).
 *   - Se arr[mid] === target, retornamos mid. Correto.
 *   - Se arr[mid] < target: como arr é ordenado, target não pode estar em arr[lo..mid].
 *     Definimos lo = mid + 1. A invariante se mantém para arr[mid+1..hi].
 *   - Se arr[mid] > target: analogamente, definimos hi = mid - 1.
 *     A invariante se mantém para arr[lo..mid-1].
 *
 * Terminação: A cada iteração, (hi - lo) diminui em pelo menos 1.
 *   Quando lo > hi, o intervalo é vazio — target não está no array.
 *   Retornamos -1. Correto.
 */
function binarySearch(arr: number[], target: number): number {
  let lo = 0;
  let hi = arr.length - 1;

  while (lo <= hi) {
    const mid = lo + Math.floor((hi - lo) / 2); // Evita overflow de (lo+hi)/2
    if (arr[mid] === target) return mid;
    if (arr[mid] < target) lo = mid + 1;
    else hi = mid - 1;
  }

  return -1;
}
```

### 1.7 Aplicação: Prova de Correção de Merge Sort

```typescript
/**
 * Merge Sort — ordena um array usando divisão e conquista.
 *
 * PROVA DE CORREÇÃO POR INDUÇÃO FORTE sobre o tamanho n do array:
 *
 * Base (n ≤ 1): Um array com 0 ou 1 elementos já está ordenado. ✓
 *
 * Passo indutivo: Suponha que mergeSort ordena corretamente arrays de tamanho < n.
 *   - Dividimos arr em left (tamanho ⌊n/2⌋) e right (tamanho ⌈n/2⌉).
 *   - Como ambos têm tamanho < n, pela hipótese de indução forte,
 *     mergeSort(left) e mergeSort(right) produzem arrays ordenados.
 *   - A função merge combina dois arrays ordenados em um array ordenado.
 *     (Isso pode ser provado por indução separada sobre |left| + |right|.)
 *   - Logo, o resultado final é ordenado. ✓
 *
 * Terminação: Cada chamada recursiva opera sobre um array estritamente menor.
 *   A recursão atinge a base quando n ≤ 1.
 */
function mergeSort(arr: number[]): number[] {
  if (arr.length <= 1) return arr;

  const mid = Math.floor(arr.length / 2);
  const left = mergeSort(arr.slice(0, mid));
  const right = mergeSort(arr.slice(mid));

  return merge(left, right);
}

function merge(left: number[], right: number[]): number[] {
  const result: number[] = [];
  let i = 0, j = 0;

  while (i < left.length && j < right.length) {
    if (left[i] <= right[j]) {
      result.push(left[i++]);
    } else {
      result.push(right[j++]);
    }
  }

  while (i < left.length) result.push(left[i++]);
  while (j < right.length) result.push(right[j++]);

  return result;
}
```

### 1.8 Resumo das Variantes de Indução

| Variante | Hipótese de Indução | Quando Usar |
|---|---|---|
| Indução fraca | P(k) → P(k+1) | Quando P(k+1) depende apenas de P(k) |
| Indução forte | P(n₀) ∧ ... ∧ P(k) → P(k+1) | Quando P(k+1) depende de valores anteriores (ex: divisão) |
| Indução estrutural | P vale para sub-estruturas → P vale para a estrutura | Provas sobre árvores, listas, expressões recursivas |

---

## 2. Combinatória

Combinatória é a matemática da contagem. Em ciência da computação, ela quantifica o tamanho de espaços de busca, o número de estados possíveis de um sistema, e o número de operações que um algoritmo brute-force precisa realizar.

### 2.1 Princípios Fundamentais de Contagem

**Regra do Produto (Princípio Multiplicativo)**: Se uma tarefa consiste em k etapas, onde a etapa i tem nᵢ opções independentes, o número total de maneiras de completar a tarefa é n₁ · n₂ · ... · nₖ.

**Exemplo**: Uma senha com 8 caracteres, usando letras minúsculas (26) e dígitos (10), tem 36⁸ = 2.821.109.907.456 possibilidades. Se adicionarmos letras maiúsculas (26) e símbolos (10), temos 72⁸ ≈ 7.22 × 10¹⁴ — um aumento de 256× na segurança.

**Regra da Soma (Princípio Aditivo)**: Se uma tarefa pode ser feita de k maneiras mutuamente exclusivas, onde a maneira i tem nᵢ opções, o total é n₁ + n₂ + ... + nₖ.

**Exemplo em CS**: Quantas funções TypeScript com 1 parâmetro ou 2 parâmetros podemos definir se temos 5 tipos disponíveis? Funções com 1 parâmetro: 5 (tipo do parâmetro) × 5 (tipo de retorno) = 25. Com 2 parâmetros: 5² × 5 = 125. Total: 25 + 125 = 150.

### 2.2 Permutações

Uma **permutação** é um arranjo ordenado de elementos.

**Permutações de n elementos tomados r de cada vez**:

```
P(n, r) = n! / (n - r)!
```

| n | r | P(n,r) | Significado |
|---|---|---|---|
| 5 | 5 | 120 | Todas as ordens de 5 elementos |
| 10 | 3 | 720 | Escolher e ordenar 3 de 10 |
| 26 | 4 | 358.800 | Palavras de 4 letras sem repetição |
| 52 | 5 | 311.875.200 | Mãos de poker (ordenadas) |

**Permutações com repetição**: Se temos n objetos onde n₁ são de tipo 1, n₂ de tipo 2, ..., nₖ de tipo k (com n₁ + n₂ + ... + nₖ = n), o número de permutações distintas é:

```
n! / (n₁! · n₂! · ... · nₖ!)
```

**Exemplo**: Quantos anagramas distintos tem a palavra "MISSISSIPPI"?

```
11! / (1! · 4! · 4! · 2!) = 39.916.800 / (1 · 24 · 24 · 2) = 34.650
```

### 2.3 Combinações

Uma **combinação** é uma seleção não-ordenada de elementos.

```
C(n, r) = n! / (r! · (n - r)!) = P(n, r) / r!
```

A notação alternativa é (n choose r) = C(n, r), também escrita como ⁿCᵣ.

**Identidades fundamentais**:

```
C(n, r) = C(n, n - r)                          [simetria]
C(n, 0) = C(n, n) = 1                          [casos extremos]
C(n, 1) = C(n, n-1) = n                        [escolher 1 ou n-1]
C(n, r) = C(n-1, r-1) + C(n-1, r)             [recorrência de Pascal]
∑ᵣ₌₀ⁿ C(n, r) = 2ⁿ                             [total de subconjuntos]
∑ᵣ₌₀ⁿ (-1)ʳ C(n, r) = 0                        [alternante]
```

A identidade ∑ C(n, r) = 2ⁿ é particularmente importante: ela diz que **um conjunto de n elementos tem exatamente 2ⁿ subconjuntos** (incluindo o vazio e o próprio conjunto). Isso explica por que algoritmos brute-force que enumeram todos os subconjuntos são O(2ⁿ).

### 2.4 Triângulo de Pascal e Coeficiente Binomial

O **Triângulo de Pascal** é construído pela recorrência C(n, r) = C(n-1, r-1) + C(n-1, r):

```
n=0:                    1
n=1:                  1   1
n=2:                1   2   1
n=3:              1   3   3   1
n=4:            1   4   6   4   1
n=5:          1   5  10  10   5   1
n=6:        1   6  15  20  15   6   1
```

O **Teorema Binomial** conecta combinações com álgebra:

```
(a + b)ⁿ = ∑ᵣ₌₀ⁿ C(n, r) · aⁿ⁻ʳ · bʳ
```

**Aplicação**: O número de caminhos em um grid m × n (movendo apenas para direita ou para baixo) de (0,0) até (m,n) é C(m+n, m) = C(m+n, n), pois precisamos escolher m movimentos "para baixo" entre m+n movimentos totais.

### 2.5 Implementação: Triângulo de Pascal com Programação Dinâmica

```typescript
/**
 * Calcula C(n, r) usando o Triângulo de Pascal (programação dinâmica).
 * Complexidade: O(n·r) tempo, O(r) espaço (otimizado com uma única linha).
 *
 * Vantagem sobre a fórmula direta n!/(r!(n-r)!):
 * - Evita overflow intermediário (fatorial cresce explosivamente)
 * - Usa apenas somas, que são mais estáveis numericamente
 */
function binomial(n: number, r: number): number {
  if (r < 0 || r > n) return 0;
  if (r === 0 || r === n) return 1;

  // Otimização: C(n, r) = C(n, n-r)
  if (r > n - r) r = n - r;

  // dp[j] representa C(i, j) para a linha atual i
  const dp = new Array(r + 1).fill(0);
  dp[0] = 1;

  for (let i = 1; i <= n; i++) {
    // Percorre de trás para frente para não sobrescrever valores necessários
    for (let j = Math.min(i, r); j > 0; j--) {
      dp[j] = dp[j] + dp[j - 1]; // C(i, j) = C(i-1, j) + C(i-1, j-1)
    }
  }

  return dp[r];
}

// Testes
console.log(binomial(5, 2));   // 10
console.log(binomial(10, 3));  // 120
console.log(binomial(20, 10)); // 184756
console.log(binomial(52, 5));  // 2598960 (mãos de poker)

/**
 * Gera o triângulo de Pascal completo até a linha n.
 */
function pascalTriangle(n: number): number[][] {
  const triangle: number[][] = [[1]];

  for (let i = 1; i <= n; i++) {
    const row = [1];
    for (let j = 1; j < i; j++) {
      row.push(triangle[i - 1][j - 1] + triangle[i - 1][j]);
    }
    row.push(1);
    triangle.push(row);
  }

  return triangle;
}

console.log(pascalTriangle(6));
// [[1], [1,1], [1,2,1], [1,3,3,1], [1,4,6,4,1], [1,5,10,10,5,1], [1,6,15,20,15,6,1]]
```

### 2.6 Geração de Todas as Permutações (Backtracking)

```typescript
/**
 * Gera todas as permutações de um array usando backtracking.
 * Complexidade: O(n! · n) — n! permutações, cada uma custa O(n) para copiar.
 *
 * O algoritmo funciona fixando cada elemento na primeira posição
 * e recursivamente permutando o resto. A técnica de swap evita
 * alocações extras — é um padrão clássico em combinatória computacional.
 */
function permutations<T>(arr: T[]): T[][] {
  const result: T[][] = [];

  function backtrack(start: number): void {
    if (start === arr.length) {
      result.push([...arr]); // Copia o estado atual
      return;
    }

    for (let i = start; i < arr.length; i++) {
      // Coloca arr[i] na posição start
      [arr[start], arr[i]] = [arr[i], arr[start]];
      backtrack(start + 1);
      // Desfaz o swap (backtrack)
      [arr[start], arr[i]] = [arr[i], arr[start]];
    }
  }

  backtrack(0);
  return result;
}

console.log(permutations([1, 2, 3]));
// [[1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,2,1], [3,1,2]]
console.log(permutations([1, 2, 3]).length); // 6 = 3!

/**
 * Gera todas as combinações C(n, r) — subconjuntos de tamanho r.
 */
function combinations<T>(arr: T[], r: number): T[][] {
  const result: T[][] = [];

  function backtrack(start: number, current: T[]): void {
    if (current.length === r) {
      result.push([...current]);
      return;
    }

    // Poda: se não há elementos suficientes restantes, para
    const remaining = arr.length - start;
    const needed = r - current.length;
    if (remaining < needed) return;

    for (let i = start; i < arr.length; i++) {
      current.push(arr[i]);
      backtrack(i + 1, current);
      current.pop(); // Backtrack
    }
  }

  backtrack(0, []);
  return result;
}

console.log(combinations([1, 2, 3, 4], 2));
// [[1,2], [1,3], [1,4], [2,3], [2,4], [3,4]]
console.log(combinations([1, 2, 3, 4], 2).length); // 6 = C(4,2)
```

### 2.7 Stars and Bars

O problema "Stars and Bars" responde: de quantas maneiras podemos distribuir n objetos **indistinguíveis** em k caixas **distinguíveis**?

A resposta é **C(n + k - 1, k - 1)**.

A intuição: represente os n objetos como estrelas (★) e as divisões entre caixas como barras (|). Por exemplo, distribuir 5 objetos em 3 caixas:

```
★★|★★★|     → (2, 3, 0)
★|★|★★★     → (1, 1, 3)
|★★★★★|     → (0, 5, 0)
||||★★★★★   — inválido (4 barras para 3 caixas — seriam 5 caixas)
```

Temos n + k - 1 = 5 + 3 - 1 = 7 posições, e precisamos escolher k - 1 = 2 delas para as barras: C(7, 2) = 21 distribuições.

**Aplicação em CS**: Quantas soluções inteiras não-negativas tem a equação x₁ + x₂ + x₃ + x₄ = 10? Resposta: C(10 + 4 - 1, 4 - 1) = C(13, 3) = 286. Isso aparece na análise de como distribuir carga entre servidores, particionar dados, ou contar o número de multisets.

### 2.8 Princípio de Inclusão-Exclusão

Para dois conjuntos:

```
|A ∪ B| = |A| + |B| - |A ∩ B|
```

Para três conjuntos:

```
|A ∪ B ∪ C| = |A| + |B| + |C| - |A∩B| - |A∩C| - |B∩C| + |A∩B∩C|
```

A forma geral para n conjuntos:

```
|A₁ ∪ A₂ ∪ ... ∪ Aₙ| = ∑|Aᵢ| - ∑|Aᵢ∩Aⱼ| + ∑|Aᵢ∩Aⱼ∩Aₖ| - ... + (-1)ⁿ⁺¹|A₁∩...∩Aₙ|
```

**Aplicação clássica — Derangements (permutações sem ponto fixo)**:

Um derangement é uma permutação onde nenhum elemento está na sua posição original. O número de derangements de n elementos é:

```
Dₙ = n! · ∑ₖ₌₀ⁿ (-1)ᵏ / k! ≈ n! / e
```

Para n = 5: D₅ = 120 · (1 - 1 + 1/2 - 1/6 + 1/24 - 1/120) = 120 · (44/120) = 44.

**Aplicação em CS**: A probabilidade de que uma permutação aleatória seja um derangement converge para 1/e ≈ 36.8%. Isso aparece na análise de algoritmos de shuffling e em problemas de matching.

```typescript
/**
 * Conta o número de derangements de n elementos.
 * Usa a recorrência: D(n) = (n-1) · (D(n-1) + D(n-2))
 * com D(0) = 1, D(1) = 0.
 */
function derangements(n: number): number {
  if (n === 0) return 1;
  if (n === 1) return 0;

  let prev2 = 1; // D(0)
  let prev1 = 0; // D(1)

  for (let i = 2; i <= n; i++) {
    const current = (i - 1) * (prev1 + prev2);
    prev2 = prev1;
    prev1 = current;
  }

  return prev1;
}

console.log(derangements(5));  // 44
console.log(derangements(10)); // 1334961
// Fração de derangements: D(10)/10! = 1334961/3628800 ≈ 0.3679 ≈ 1/e
```

---

## 3. Probabilidade Discreta

Probabilidade discreta é a ferramenta matemática para analisar algoritmos randomizados (quicksort randomizado, hashing universal, skip lists), estimar performance esperada e compreender fenômenos como o Birthday Paradox.

### 3.1 Espaço Amostral, Eventos e Probabilidade

O **espaço amostral** S é o conjunto de todos os resultados possíveis de um experimento. Um **evento** E é um subconjunto de S. A **probabilidade** de E é:

```
P(E) = |E| / |S|     (para espaços amostrais finitos com resultados equiprováveis)
```

**Axiomas de Kolmogorov**:
1. P(E) ≥ 0 para todo evento E
2. P(S) = 1
3. Para eventos mutuamente exclusivos E₁, E₂, ...: P(E₁ ∪ E₂ ∪ ...) = P(E₁) + P(E₂) + ...

**Consequências**:
- P(Eᶜ) = 1 - P(E) (complemento)
- P(A ∪ B) = P(A) + P(B) - P(A ∩ B) (inclusão-exclusão)
- 0 ≤ P(E) ≤ 1

### 3.2 Probabilidade Condicional e Teorema de Bayes

A **probabilidade condicional** de A dado que B ocorreu:

```
P(A|B) = P(A ∩ B) / P(B),    para P(B) > 0
```

**Regra da cadeia**: P(A ∩ B) = P(A|B) · P(B) = P(B|A) · P(A)

**Teorema de Bayes**:

```
P(A|B) = P(B|A) · P(A) / P(B)
```

Onde P(B) pode ser expandido pela **lei da probabilidade total**:

```
P(B) = P(B|A) · P(A) + P(B|Aᶜ) · P(Aᶜ)
```

**Exemplo prático — Testes de software**:

Um sistema tem taxa de defeitos de 1% (P(D) = 0.01). Um teste automatizado detecta defeitos com 95% de acurácia (P(T⁺|D) = 0.95) e tem 3% de falsos positivos (P(T⁺|Dᶜ) = 0.03). Se o teste dá positivo, qual a probabilidade de haver um defeito real?

```
P(D|T⁺) = P(T⁺|D) · P(D) / P(T⁺)
         = P(T⁺|D) · P(D) / [P(T⁺|D)·P(D) + P(T⁺|Dᶜ)·P(Dᶜ)]
         = (0.95 · 0.01) / (0.95 · 0.01 + 0.03 · 0.99)
         = 0.0095 / (0.0095 + 0.0297)
         = 0.0095 / 0.0392
         ≈ 0.242 = 24.2%
```

Um resultado positivo no teste indica defeito real apenas 24.2% das vezes. Isso é chamado **base rate fallacy** — quando a prevalência base é baixa, mesmo testes com boa acurácia produzem muitos falsos positivos. **Implicação em monitoring**: alarmes em sistemas de produção com baixa taxa de falhas reais terão alta proporção de falsos positivos.

### 3.3 Independência de Eventos

Dois eventos A e B são **independentes** se e somente se:

```
P(A ∩ B) = P(A) · P(B)
```

Equivalentemente: P(A|B) = P(A) e P(B|A) = P(B) — saber que um ocorreu não muda a probabilidade do outro.

**Cuidado**: Independência ≠ eventos mutuamente exclusivos. Na verdade, se A e B são mutuamente exclusivos com P(A) > 0 e P(B) > 0, eles são **dependentes** (saber que A ocorreu implica que B não ocorreu).

**Aplicação**: A probabilidade de k lançamentos de moeda serem todos cara: P = (1/2)ᵏ. Após 10 caras consecutivas, a probabilidade da próxima ser cara ainda é 1/2 (os lançamentos são independentes). A falácia do jogador ignora isso.

### 3.4 Variáveis Aleatórias e Valor Esperado

Uma **variável aleatória** (v.a.) discreta X é uma função X: S → ℝ que associa cada resultado do espaço amostral a um número real.

O **valor esperado** (esperança) de X:

```
E[X] = ∑ₓ x · P(X = x)
```

**Propriedade crucial — Linearidade da Esperança**:

```
E[X + Y] = E[X] + E[Y]
```

Essa identidade vale **mesmo que X e Y sejam dependentes**. É uma das ferramentas mais poderosas em análise de algoritmos.

**Exemplo — Número esperado de comparações no quicksort randomizado**:

Defina Xᵢⱼ = 1 se os elementos i e j são comparados, 0 caso contrário. O total de comparações é X = ∑ᵢ&lt;ⱼ Xᵢⱼ. Pela linearidade:

```
E[X] = ∑ᵢ<ⱼ E[Xᵢⱼ] = ∑ᵢ<ⱼ P(i e j são comparados)
```

Dois elementos com ranks i e j (onde j - i = k) são comparados se e somente se um deles é o **primeiro** a ser escolhido como pivot entre os elementos de rank i, i+1, ..., j. A probabilidade disso é 2/(j - i + 1). Somando:

```
E[X] = ∑ᵢ₌₁ⁿ ∑ⱼ₌ᵢ₊₁ⁿ 2/(j-i+1) = ∑ᵢ₌₁ⁿ ∑ₖ₌₁ⁿ⁻ⁱ 2/(k+1) ≤ ∑ᵢ₌₁ⁿ 2·Hₙ = 2n·Hₙ ≈ 2n·ln(n)
```

Onde Hₙ = ∑ₖ₌₁ⁿ 1/k ≈ ln(n) é o n-ésimo número harmônico. Logo, o **tempo esperado** do quicksort randomizado é **O(n log n)**.

### 3.5 Birthday Paradox

O Birthday Paradox é um dos resultados mais contraintuitivos e relevantes em ciência da computação. A pergunta: em um grupo de n pessoas, qual a probabilidade de pelo menos duas terem o mesmo aniversário (assumindo 365 dias equiprováveis)?

```
P(colisão) = 1 - P(sem colisão)
           = 1 - (365/365) · (364/365) · (363/365) · ... · ((365-n+1)/365)
           = 1 - ∏ᵢ₌₀ⁿ⁻¹ (1 - i/365)
```

| n | P(colisão) |
|---|---|
| 10 | 11.7% |
| 23 | **50.7%** |
| 30 | 70.6% |
| 50 | 97.0% |
| 70 | 99.9% |

Com apenas 23 pessoas, a probabilidade de colisão já ultrapassa 50%.

**Forma geral**: Para n itens distribuídos uniformemente em m slots:

```
P(colisão) ≈ 1 - e^(-n²/(2m))
```

O número de inserções até 50% de probabilidade de colisão é aproximadamente √(π·m/2) ≈ 1.177√m.

**Conexão com Hash Tables**: Uma hash table com m = 1.000.000 slots sofrerá a primeira colisão, em média, após ~1.177 inserções — muito antes de preencher 1% da tabela. Isso demonstra que **estratégias de resolução de colisão não são opcionais**, mesmo para tabelas grandes.

### 3.6 Simulação do Birthday Paradox

```typescript
/**
 * Simula o Birthday Paradox para determinar empiricamente
 * quantas inserções são necessárias até a primeira colisão.
 */
function birthdayParadoxSimulation(
  slots: number,
  trials: number = 100_000
): { avgInsertions: number; theoretical: number } {
  let totalInsertions = 0;

  for (let t = 0; t < trials; t++) {
    const seen = new Set<number>();
    let n = 0;

    while (true) {
      const slot = Math.floor(Math.random() * slots);
      if (seen.has(slot)) break;
      seen.add(slot);
      n++;
    }

    totalInsertions += n;
  }

  const avgInsertions = totalInsertions / trials;
  const theoretical = Math.sqrt((Math.PI * slots) / 2);

  return { avgInsertions, theoretical };
}

// Simulação com 365 "slots" (dias do ano)
const result365 = birthdayParadoxSimulation(365, 50_000);
console.log(`Dias: avg=${result365.avgInsertions.toFixed(1)}, teórico=${result365.theoretical.toFixed(1)}`);
// Dias: avg≈24.6, teórico≈23.9

// Simulação com 1.000.000 slots (hash table)
const result1M = birthdayParadoxSimulation(1_000_000, 10_000);
console.log(`Hash 1M: avg=${result1M.avgInsertions.toFixed(0)}, teórico=${result1M.theoretical.toFixed(0)}`);
// Hash 1M: avg≈1177, teórico≈1253
```

### 3.7 Estimativa de Monte Carlo para π

A técnica de Monte Carlo usa amostragem aleatória para estimar valores. Um exemplo clássico é estimar π:

Inscrevemos um círculo de raio 1 em um quadrado de lado 2. A razão das áreas é:

```
Área do círculo / Área do quadrado = π·r² / (2r)² = π/4
```

Gerando pontos aleatórios no quadrado e contando quantos caem dentro do círculo:

```
π ≈ 4 · (pontos dentro do círculo) / (total de pontos)
```

```typescript
/**
 * Estima π usando o método de Monte Carlo.
 * Gera pontos aleatórios em [0,1]×[0,1] e verifica se x²+y² ≤ 1
 * (um quarto do círculo unitário inscrito no quadrado unitário).
 *
 * A convergência é O(1/√n) — precisamos 100× mais pontos para
 * cada dígito adicional de precisão. Isso é lento para π,
 * mas Monte Carlo é essencial para integrais de alta dimensão
 * onde métodos determinísticos sofrem da "curse of dimensionality".
 */
function estimatePi(numPoints: number): number {
  let insideCircle = 0;

  for (let i = 0; i < numPoints; i++) {
    const x = Math.random();
    const y = Math.random();
    if (x * x + y * y <= 1) {
      insideCircle++;
    }
  }

  return 4 * insideCircle / numPoints;
}

// Testes com diferentes quantidades de pontos
for (const n of [1_000, 10_000, 100_000, 1_000_000]) {
  const estimate = estimatePi(n);
  const error = Math.abs(estimate - Math.PI);
  console.log(`n=${n.toLocaleString().padStart(9)}: π ≈ ${estimate.toFixed(6)}, erro = ${error.toFixed(6)}`);
}
// Saída típica:
// n=    1,000: π ≈ 3.148000, erro = 0.006407
// n=   10,000: π ≈ 3.137600, erro = 0.003993
// n=  100,000: π ≈ 3.141280, erro = 0.000313
// n=1,000,000: π ≈ 3.141784, erro = 0.000191
```

### 3.8 Distribuições Importantes em CS

| Distribuição | P(X = k) | E[X] | Aplicação em CS |
|---|---|---|---|
| Bernoulli(p) | P(1)=p, P(0)=1-p | p | Sucesso/falha de uma operação |
| Binomial(n, p) | C(n,k)·pᵏ·(1-p)ⁿ⁻ᵏ | np | Número de erros em n pacotes |
| Geométrica(p) | (1-p)ᵏ⁻¹·p | 1/p | Tentativas até primeiro sucesso (ex: retry) |
| Uniforme(1, n) | 1/n | (n+1)/2 | Seleção aleatória de pivot no quicksort |
| Poisson(λ) | e⁻λ·λᵏ/k! | λ | Número de requests em um intervalo |

A **distribuição geométrica** é especialmente útil: se cada tentativa tem probabilidade p de sucesso, o número esperado de tentativas até o primeiro sucesso é 1/p. Isso aparece na análise de:
- Hashing: tentativas até encontrar um slot vazio
- Retries com backoff: tentativas até sucesso
- Skip lists: número de níveis de um nó

---

## 4. Relações e Funções

Relações e funções são os blocos de construção da matemática formal e têm aplicações diretas em estruturas de dados, sistemas de tipos e modelagem de dados.

### 4.1 Relações Binárias

Uma **relação binária** R de um conjunto A para um conjunto B é um subconjunto de A × B (o produto cartesiano). Se (a, b) ∈ R, escrevemos aRb.

**Exemplo**: Seja A = `{1, 2, 3, 4}` e defina a relação "divide" como R = `{(a, b) | a divide b}`:

```
R = {(1,1), (1,2), (1,3), (1,4), (2,2), (2,4), (3,3), (4,4)}
```

### 4.2 Propriedades de Relações em A × A

| Propriedade | Definição Formal | Exemplo (em ℤ) |
|---|---|---|
| Reflexiva | ∀a ∈ A: aRa | a ≤ a ✓ |
| Irreflexiva | ∀a ∈ A: ¬(aRa) | a &lt; a (nunca) ✓ |
| Simétrica | ∀a,b: aRb → bRa | a = b → b = a ✓ |
| Antissimétrica | ∀a,b: (aRb ∧ bRa) → a = b | (a ≤ b ∧ b ≤ a) → a = b ✓ |
| Transitiva | ∀a,b,c: (aRb ∧ bRc) → aRc | (a ≤ b ∧ b ≤ c) → a ≤ c ✓ |

### 4.3 Relações de Equivalência

Uma relação que é **reflexiva**, **simétrica** e **transitiva** é uma **relação de equivalência**. Toda relação de equivalência em A induz uma **partição** de A em **classes de equivalência** mutuamente disjuntas.

**Teorema Fundamental**: Existe uma bijeção entre relações de equivalência em A e partições de A.

**Exemplos**:

1. **Congruência módulo n**: a ≡ b (mod n) é uma relação de equivalência. As classes são `{0, n, 2n, ...}`, `{1, n+1, 2n+1, ...}`, etc.

2. **Igualdade de tipos**: Em TypeScript, a relação "tem o mesmo tipo estrutural" é uma relação de equivalência sobre valores.

3. **Componentes conexos**: Em um grafo não-direcionado, "estar no mesmo componente conexo" é uma relação de equivalência sobre vértices. As classes de equivalência são exatamente os componentes conexos.

### 4.4 Aplicação: Union-Find (Disjoint Sets)

A estrutura Union-Find mantém uma coleção de conjuntos disjuntos (uma partição) e suporta:

- **Find(x)**: Retorna o representante (root) da classe de equivalência de x.
- **Union(x, y)**: Funde as classes de equivalência de x e y.

Com **path compression** e **union by rank**, ambas as operações são O(α(n)) amortizado, onde α é a inversa da função de Ackermann — efetivamente O(1) para qualquer entrada prática (α(n) ≤ 4 para n ≤ 10⁸⁰).

```typescript
/**
 * Union-Find com path compression e union by rank.
 *
 * Mantém uma relação de equivalência dinâmica sobre {0, 1, ..., n-1}.
 * Aplicações: componentes conexos (Kruskal), detecção de ciclos,
 * equivalência de variáveis em compiladores, percolação.
 */
class UnionFind {
  private parent: number[];
  private rank: number[];
  private _components: number;

  constructor(n: number) {
    // Inicialmente, cada elemento é seu próprio representante
    this.parent = Array.from({ length: n }, (_, i) => i);
    this.rank = new Array(n).fill(0);
    this._components = n;
  }

  /**
   * Encontra o representante (root) da classe de equivalência de x.
   * Path compression: faz todos os nós no caminho apontarem diretamente para a root.
   * Isso "achata" a árvore, garantindo operações futuras em tempo quase constante.
   */
  find(x: number): number {
    if (this.parent[x] !== x) {
      this.parent[x] = this.find(this.parent[x]); // Path compression
    }
    return this.parent[x];
  }

  /**
   * Funde as classes de equivalência de x e y.
   * Union by rank: anexa a árvore mais rasa à mais profunda.
   * Isso mantém a profundidade em O(log n) no pior caso.
   */
  union(x: number, y: number): boolean {
    const rootX = this.find(x);
    const rootY = this.find(y);

    if (rootX === rootY) return false; // Já na mesma classe

    // Union by rank
    if (this.rank[rootX] < this.rank[rootY]) {
      this.parent[rootX] = rootY;
    } else if (this.rank[rootX] > this.rank[rootY]) {
      this.parent[rootY] = rootX;
    } else {
      this.parent[rootY] = rootX;
      this.rank[rootX]++;
    }

    this._components--;
    return true;
  }

  /**
   * Verifica se x e y estão na mesma classe de equivalência.
   */
  connected(x: number, y: number): boolean {
    return this.find(x) === this.find(y);
  }

  get components(): number {
    return this._components;
  }
}

// Exemplo: componentes conexos
const uf = new UnionFind(7);
// Arestas: (0,1), (1,2), (3,4), (5,6)
uf.union(0, 1);
uf.union(1, 2);
uf.union(3, 4);
uf.union(5, 6);

console.log(uf.components);       // 3 componentes: {0,1,2}, {3,4}, {5,6}
console.log(uf.connected(0, 2));  // true
console.log(uf.connected(0, 3));  // false

// Conectar dois componentes
uf.union(2, 4);
console.log(uf.components);       // 2 componentes: {0,1,2,3,4}, {5,6}
console.log(uf.connected(0, 3));  // true
```

### 4.5 Relações de Ordem Parcial

Uma relação que é **reflexiva**, **antissimétrica** e **transitiva** é uma **ordem parcial** (poset — partially ordered set).

**Exemplos**:
- ≤ sobre os inteiros (ordem total — todo par é comparável)
- ⊆ sobre subconjuntos de um conjunto (ordem parcial — nem todo par é comparável)
- Divisibilidade a|b sobre os naturais
- **Dependência entre tarefas** (grafo de dependências)

Em uma ordem parcial, nem todo par de elementos precisa ser comparável. Se a e b não são comparáveis (nem a ≤ b nem b ≤ a), são **incomparáveis**.

**Ordem total**: Uma ordem parcial onde todo par é comparável. Exemplo: ≤ sobre ℝ.

### 4.6 Aplicação: Ordenação Topológica

Um **DAG** (Directed Acyclic Graph) define uma ordem parcial sobre seus vértices. A **ordenação topológica** produz uma sequência linear compatível com essa ordem: se há aresta u → v, então u aparece antes de v.

```
Dependências de compilação:
  A → B (A depende de B)
  A → C
  B → D
  C → D

Ordenação topológica: D, B, C, A (ou D, C, B, A)
```

A existência de ordenação topológica é equivalente ao grafo ser acíclico. Se há ciclo, a ordem parcial "colapsa" (teríamos a ≤ b ∧ b ≤ a com a ≠ b, violando antissimetria).

**Aplicações em CS**:
- Resolução de dependências (npm, make, compiladores)
- Escalonamento de tarefas
- Serialização de transações em bancos de dados
- Ordem de avaliação em spreadsheets

### 4.7 Funções: Injetora, Sobrejetora, Bijetora

Uma **função** f: A → B é uma relação onde cada a ∈ A está associado a **exatamente um** b ∈ B.

| Tipo | Definição | Propriedade | Exemplo |
|---|---|---|---|
| Injetora (1-to-1) | f(a₁) = f(a₂) → a₁ = a₂ | Elementos distintos mapeiam para imagens distintas | f(x) = 2x |
| Sobrejetora (onto) | ∀b ∈ B, ∃a ∈ A: f(a) = b | Todo elemento do codomínio é atingido | f: ℤ → ℤ, f(x) = x+1 |
| Bijetora | Injetora e sobrejetora | Correspondência perfeita 1-para-1 | f: &#123;0,1&#125;ⁿ → &#123;0,...,2ⁿ-1&#125; |

**Por que bijeções importam**:
1. Se existe bijeção f: A → B, então |A| = |B|. Isso é a **definição** de "mesma cardinalidade".
2. Funções hash não podem ser injetoras se |U| > |m| (Pigeonhole Principle).
3. Criptografia: funções de cifragem devem ser bijeções (para permitir decifragem).
4. Isomorfismo de estruturas: dois grafos são isomorfos se existe uma bijeção entre seus vértices que preserva arestas.

### 4.8 Funções em TypeScript — Composição e Tipos

```typescript
/**
 * Em TypeScript, funções tipadas refletem propriedades matemáticas.
 *
 * - Uma função injetora pode ser "invertida" no seu domínio
 * - Uma função sobrejetora garante que todo tipo do codomínio é produzido
 * - O sistema de tipos pode (parcialmente) garantir essas propriedades
 */

// Composição de funções: (g ∘ f)(x) = g(f(x))
function compose<A, B, C>(
  g: (b: B) => C,
  f: (a: A) => B
): (a: A) => C {
  return (a: A) => g(f(a));
}

// Exemplo: pipeline de transformação de dados
const parseAge = (s: string): number => parseInt(s, 10);
const isAdult = (age: number): boolean => age >= 18;
const canVote = compose(isAdult, parseAge);

console.log(canVote("25")); // true
console.log(canVote("16")); // false

// Bijeção: encoding/decoding
// Uma bijeção f tem inversa f⁻¹ tal que f⁻¹(f(x)) = x e f(f⁻¹(y)) = y
const encode = (s: string): string => btoa(s);  // Base64 encode
const decode = (s: string): string => atob(s);  // Base64 decode

const original = "Hello, World!";
console.log(decode(encode(original)) === original); // true — bijeção (no domínio de strings ASCII)
```

---

## 5. Princípio da Casa dos Pombos (Pigeonhole Principle)

O Pigeonhole Principle é deceptivamente simples, mas surpreendentemente poderoso. Ele permite provar a **existência** de objetos com certas propriedades sem precisar construí-los explicitamente.

### 5.1 Formulação Básica

Se **n + 1 objetos** são colocados em **n caixas**, então **pelo menos uma caixa contém 2 ou mais objetos**.

Formalmente: Se f: A → B com |A| > |B|, então f não é injetora — existem a₁ ≠ a₂ com f(a₁) = f(a₂).

### 5.2 Formulação Generalizada

Se **n objetos** são colocados em **k caixas**, então pelo menos uma caixa contém pelo menos **⌈n/k⌉** objetos.

**Prova**: Por contradição. Se toda caixa contivesse menos de ⌈n/k⌉ objetos, o total seria no máximo k · (⌈n/k⌉ - 1) &lt; k · (n/k) = n. Contradição, pois temos n objetos.

### 5.3 Colisões Inevitáveis em Hash Tables

**Teorema**: Se uma hash table tem m slots e inserimos m + 1 chaves, **pelo menos duas chaves colidem** (mapeiam para o mesmo slot), independentemente da função hash usada.

**Prova**: Direta pelo Pigeonhole Principle. A função hash `h: U → {0, ..., m-1}` mapeia m+1 objetos (chaves) em m caixas (slots). Logo, h não é injetora.

**Consequência prática**: Toda implementação de hash table **precisa** de uma estratégia de resolução de colisões. Não é uma questão de escolher uma "função hash perfeita" — é uma impossibilidade matemática evitar colisões quando |keys| > |slots|.

Além disso, pelo Birthday Paradox (seção 3.5), colisões ocorrem **muito antes** de termos mais chaves que slots. Mesmo com m = 1.000.000, esperamos a primeira colisão após ~1.177 inserções.

### 5.4 Aplicação: Repetição Garantida em Strings

**Teorema**: Qualquer string com mais de 26 caracteres (usando apenas letras minúsculas do alfabeto inglês) contém pelo menos uma letra repetida.

**Prova**: Pigeonhole. As 26 letras são as "caixas"; os caracteres da string são os "objetos". Se a string tem 27+ caracteres, pelo menos uma caixa (letra) contém 2+ objetos (ocorrências).

**Generalização**: Uma string de comprimento n sobre um alfabeto de tamanho σ contém pelo menos uma repetição se n > σ. A letra mais frequente aparece pelo menos ⌈n/σ⌉ vezes.

```typescript
/**
 * Demonstra o Pigeonhole Principle em strings:
 * encontra a primeira letra que se repete.
 */
function findFirstRepeatedChar(s: string): string | null {
  const seen = new Set<string>();
  for (const char of s) {
    if (seen.has(char)) return char;
    seen.add(char);
  }
  return null;
}

// Pelo Pigeonhole, se s.length > 26 (e s usa apenas a-z), SEMPRE há repetição
const s = "abcdefghijklmnopqrstuvwxyz"; // 26 caracteres, sem repetição (possível)
console.log(findFirstRepeatedChar(s));   // null

const s2 = "abcdefghijklmnopqrstuvwxyza"; // 27 caracteres → repetição garantida
console.log(findFirstRepeatedChar(s2));    // "a"

/**
 * Encontra um duplicado em um array de n+1 inteiros no intervalo [1, n].
 * Pelo Pigeonhole Principle, um duplicado SEMPRE existe.
 *
 * O algoritmo de Floyd (tartaruga e lebre) resolve em O(n) tempo e O(1) espaço,
 * sem modificar o array. Trata o array como um grafo funcional
 * e detecta o ciclo (que corresponde ao duplicado).
 */
function findDuplicate(nums: number[]): number {
  // Fase 1: Detectar ciclo (tartaruga e lebre)
  let slow = nums[0];
  let fast = nums[0];

  do {
    slow = nums[slow];
    fast = nums[nums[fast]];
  } while (slow !== fast);

  // Fase 2: Encontrar entrada do ciclo
  slow = nums[0];
  while (slow !== fast) {
    slow = nums[slow];
    fast = nums[fast];
  }

  return slow;
}

// Exemplo: array com 5+1=6 elementos no intervalo [1, 5]
console.log(findDuplicate([1, 3, 4, 2, 2])); // 2
console.log(findDuplicate([3, 1, 3, 4, 2])); // 3
```

### 5.5 Aplicação: Teorema de Ramsey (versão simplificada)

**Teorema**: Em qualquer grupo de 6 pessoas, existem 3 que se conhecem mutuamente **ou** 3 que não se conhecem mutuamente.

**Prova usando Pigeonhole**:

Considere uma pessoa qualquer, digamos Alice. Ela tem 5 relações (com as outras 5 pessoas), cada uma de dois tipos: "conhece" ou "não conhece". Pelo Pigeonhole (5 relações em 2 tipos), pelo menos ⌈5/2⌉ = 3 relações são do mesmo tipo.

**Caso 1**: Alice conhece pelo menos 3 pessoas, digamos B, C, D.
- Se B, C e D se conhecem mutuamente entre si (qualquer par), temos um triângulo de "conhecidos" (incluindo Alice com qualquer um deles — na verdade, se B e C se conhecem, B e D se conhecem, e C e D se conhecem, então &#123;B, C, D&#125; formam o trio).
- Precisamos apenas que algum par entre &#123;B, C, D&#125; se conheça para formar um trio com Alice. Se nenhum par se conhece, então &#123;B, C, D&#125; formam um trio de desconhecidos.

Mais precisamente: entre B, C, D, se algum par se conhece (digamos B e C), então &#123;Alice, B, C&#125; se conhecem mutuamente. Se nenhum par entre B, C, D se conhece, então &#123;B, C, D&#125; não se conhecem mutuamente.

**Caso 2**: Alice não conhece pelo menos 3 pessoas. Análogo ao Caso 1.

Esse resultado é R(3,3) = 6, o menor número de Ramsey para triângulos. Números de Ramsey são extremamente difíceis de calcular — R(5,5) é desconhecido até hoje.

**Conexão com CS**: Teoria de Ramsey aparece em verificação formal, networking (protocolos de consenso) e em provas de que certos padrões são inevitáveis em estruturas suficientemente grandes.

### 5.6 Aplicação em Roteamento de Rede

**Teorema**: Em uma rede com n nós, se um pacote visita mais de n nós distintos, ele está em um loop.

**Prova**: Pigeonhole. Um pacote que visita n+1 nós em uma rede de n nós deve ter visitado algum nó duas vezes — ou seja, entrou em um loop.

**Aplicação prática**: O campo **TTL (Time To Live)** em pacotes IP é um contador que decrementa a cada hop. Se chega a 0, o pacote é descartado. Isso previne loops infinitos na rede. O TTL padrão é tipicamente 64 ou 128, baseado na premissa (Pigeonhole) de que qualquer caminho legítimo na Internet atravessa menos nós que o TTL.

```typescript
/**
 * Simula roteamento com detecção de loop baseada no Pigeonhole Principle.
 * Se visitamos mais nós do que existem na rede, há um loop.
 */
function routePacket(
  network: Map<string, string>, // nó → próximo nó
  start: string,
  destination: string
): { path: string[]; loopDetected: boolean } {
  const path: string[] = [start];
  const maxHops = network.size; // Pigeonhole: mais que isso implica loop
  let current = start;

  for (let hop = 0; hop < maxHops; hop++) {
    if (current === destination) {
      return { path, loopDetected: false };
    }

    const next = network.get(current);
    if (!next) {
      return { path, loopDetected: false }; // Sem rota (dead end)
    }

    path.push(next);
    current = next;
  }

  return { path, loopDetected: true }; // Pigeonhole: loop detectado
}

// Rede com loop
const network = new Map([
  ["A", "B"],
  ["B", "C"],
  ["C", "D"],
  ["D", "B"], // Loop: B → C → D → B
]);

const result = routePacket(network, "A", "E");
console.log(result.loopDetected); // true
console.log(result.path);         // ["A", "B", "C", "D", "B"]
```

---

## 6. Teoria de Grafos Formal

Grafos são a estrutura matemática mais versátil em ciência da computação. Esta seção fornece a **base teórica formal** — definições, teoremas e propriedades fundamentais. A implementação prática (representações em memória, algoritmos de busca, caminhos mínimos) é coberta na lição dedicada a Graphs.

### 6.1 Definição Formal

Um **grafo** G = (V, E) consiste em:
- V: um conjunto finito de **vértices** (nós)
- E: um conjunto de **arestas**, onde cada aresta é um par de vértices

**Grafo não-direcionado**: E ⊆ `{{u, v} | u, v ∈ V, u ≠ v}`. Cada aresta é um conjunto de 2 vértices (ordem não importa).

**Grafo direcionado (digrafo)**: E ⊆ `{(u, v) | u, v ∈ V}`. Cada aresta é um par ordenado (a direção importa).

**Terminologia básica**:

| Conceito | Definição |
|---|---|
| Adjacência | u e v são adjacentes se &#123;u,v&#125; ∈ E |
| Incidência | A aresta e é incidente em v se v ∈ e |
| Vizinhança N(v) | N(v) = &#123;u ∈ V \| &#123;u,v&#125; ∈ E&#125; |
| Loop | Aresta de v para v (self-loop) |
| Multi-aresta | Mais de uma aresta entre o mesmo par |
| Grafo simples | Sem loops e sem multi-arestas |

### 6.2 Grau de Vértice e Handshaking Lemma

O **grau** de um vértice v, denotado δ(v) ou deg(v), é o número de arestas incidentes em v.

Em um digrafo, distinguimos:
- **Grau de entrada** (in-degree): δ⁻(v) = |&#123;(u, v) ∈ E&#125;|
- **Grau de saída** (out-degree): δ⁺(v) = |&#123;(v, u) ∈ E&#125;|

**Handshaking Lemma (Lema do Aperto de Mão)**:

```
∑ᵥ∈V δ(v) = 2|E|
```

**Prova**: Cada aresta &#123;u, v&#125; contribui +1 para δ(u) e +1 para δ(v), totalizando 2 por aresta.

**Corolário**: O número de vértices com grau ímpar é sempre **par**.

**Prova**: Seja V_odd = &#123;v ∈ V | δ(v) é ímpar&#125;. Temos ∑ᵥ δ(v) = 2|E| (par). A soma dos graus pares é par. Para a soma total ser par, a soma dos graus ímpares deve ser par. Uma soma de números ímpares só é par se houver um número par de termos.

**Aplicação em CS**: Em uma rede social, o número total de "amizades" é metade da soma dos graus. Se cada usuário tem, em média, 338 amigos (como no Facebook), e há 3 bilhões de usuários, o número total de arestas é aproximadamente 3 × 10⁹ × 338 / 2 ≈ 5 × 10¹¹.

### 6.3 Caminhos, Trilhas e Ciclos

**Definições formais**:

| Conceito | Definição | Repetição de vértices | Repetição de arestas |
|---|---|---|---|
| **Walk** | Sequência v₀, e₁, v₁, e₂, ..., vₖ | Permitida | Permitida |
| **Trail** | Walk sem arestas repetidas | Permitida | Não |
| **Path (Caminho)** | Walk sem vértices repetidos | Não | Não |
| **Cycle (Ciclo)** | Walk fechado (v₀ = vₖ) sem repetição de vértices internos | Não (exceto v₀=vₖ) | Não |

**Comprimento**: O comprimento de um walk/trail/path/cycle é o número de arestas.

**Distância**: A distância d(u, v) entre u e v é o comprimento do menor caminho entre eles (∞ se não há caminho).

### 6.4 Conectividade

Um grafo não-direcionado G é **conexo** se existe um caminho entre todo par de vértices. Um **componente conexo** é um subgrafo conexo maximal.

**Propriedade**: Um grafo com n vértices precisa de pelo menos n - 1 arestas para ser conexo.

**Prova**: Cada aresta pode conectar no máximo 2 componentes. Começando com n componentes (cada vértice isolado), cada aresta reduz o número de componentes em no máximo 1. Para chegar a 1 componente, precisamos de pelo menos n - 1 reduções, ou seja, n - 1 arestas.

Para grafos direcionados, distinguimos:
- **Fortemente conexo**: existe caminho direcionado de u para v **e** de v para u, para todo par (u, v).
- **Fracamente conexo**: o grafo não-direcionado subjacente é conexo.

### 6.5 Grafos Bipartidos

Um grafo G = (V, E) é **bipartido** se V pode ser particionado em dois conjuntos disjuntos U e W tais que toda aresta conecta um vértice de U a um de W.

**Teorema**: Um grafo é bipartido se e somente se **não contém ciclos de comprimento ímpar**.

**Prova (esboço)**:
- (→) Se G é bipartido com partição (U, W), qualquer ciclo alterna entre U e W. Como deve retornar ao ponto de partida, o número de alternâncias é par → ciclo de comprimento par.
- (←) Se G não tem ciclos ímpares, podemos bipartir via BFS: coloque a raiz em U, vizinhos em W, vizinhos dos vizinhos em U, etc. A ausência de ciclos ímpares garante que nenhuma aresta conecta dois vértices do mesmo conjunto.

**Aplicações de grafos bipartidos**:
- **Matching**: atribuir tarefas a trabalhadores (bipartite matching)
- **Coloração com 2 cores**: grafos bipartidos são exatamente os grafos 2-coloráveis
- **Recomendações**: grafo usuários × produtos

```typescript
/**
 * Verifica se um grafo é bipartido usando BFS (coloração com 2 cores).
 * Retorna a bipartição se existir, ou null se o grafo não for bipartido.
 */
function isBipartite(
  adjList: Map<number, number[]>
): { setA: Set<number>; setB: Set<number> } | null {
  const color = new Map<number, 0 | 1>();
  const setA = new Set<number>();
  const setB = new Set<number>();

  // BFS para cada componente conexo
  for (const startNode of adjList.keys()) {
    if (color.has(startNode)) continue;

    const queue: number[] = [startNode];
    color.set(startNode, 0);
    setA.add(startNode);

    while (queue.length > 0) {
      const node = queue.shift()!;
      const nodeColor = color.get(node)!;
      const neighborColor: 0 | 1 = nodeColor === 0 ? 1 : 0;

      for (const neighbor of adjList.get(node) ?? []) {
        if (!color.has(neighbor)) {
          color.set(neighbor, neighborColor);
          (neighborColor === 0 ? setA : setB).add(neighbor);
          queue.push(neighbor);
        } else if (color.get(neighbor) === nodeColor) {
          // Aresta entre vértices da mesma cor → ciclo ímpar → não bipartido
          return null;
        }
      }
    }
  }

  return { setA, setB };
}

// Grafo bipartido: 0-1-2-3-0 (ciclo de comprimento 4)
const bipartite = new Map([
  [0, [1, 3]],
  [1, [0, 2]],
  [2, [1, 3]],
  [3, [2, 0]],
]);
console.log(isBipartite(bipartite)); // { setA: {0, 2}, setB: {1, 3} }

// Grafo NÃO bipartido: 0-1-2-0 (ciclo de comprimento 3)
const nonBipartite = new Map([
  [0, [1, 2]],
  [1, [0, 2]],
  [2, [0, 1]],
]);
console.log(isBipartite(nonBipartite)); // null
```

### 6.6 Árvores

Uma **árvore** é um grafo conexo acíclico. Equivalentemente, qualquer **duas** das seguintes três propriedades implicam a terceira:

1. G é conexo
2. G é acíclico
3. |E| = |V| - 1

**Propriedades de árvores**:

| Propriedade | Resultado |
|---|---|
| Caminhos | Entre quaisquer dois vértices, existe **exatamente** um caminho |
| Arestas | |E| = |V| - 1 |
| Folhas | Uma árvore com ≥ 2 vértices tem pelo menos 2 folhas (vértices de grau 1) |
| Remoção de aresta | Remover qualquer aresta desconecta a árvore |
| Adição de aresta | Adicionar qualquer aresta cria exatamente um ciclo |

**Prova de que |E| = |V| - 1** (por indução sobre |V|):

**Base**: |V| = 1 → 0 arestas = 1 - 1. ✓

**Passo**: Uma árvore com n ≥ 2 vértices tem pelo menos uma folha v (vértice de grau 1). Remova v e sua aresta incidente. O grafo resultante é uma árvore com n-1 vértices. Pela hipótese de indução, tem n-2 arestas. A árvore original tem n-2+1 = n-1 arestas. ✓

**Árvores geradoras**: Uma **árvore geradora** (spanning tree) de um grafo conexo G é um subgrafo que é árvore e contém todos os vértices de G. Todo grafo conexo tem pelo menos uma árvore geradora.

### 6.7 Planaridade

Um grafo é **planar** se pode ser desenhado no plano sem que arestas se cruzem.

**Fórmula de Euler para grafos planares conexos**:

```
V - E + F = 2
```

Onde V = número de vértices, E = número de arestas, F = número de faces (incluindo a face externa).

**Corolário**: Para um grafo planar simples conexo com V ≥ 3:

```
E ≤ 3V - 6
```

**Prova**: Cada face é delimitada por pelo menos 3 arestas, e cada aresta delimita no máximo 2 faces. Logo, 2E ≥ 3F → F ≤ 2E/3. Substituindo em Euler: V - E + 2E/3 ≥ 2 → V - E/3 ≥ 2 → E ≤ 3V - 6.

**Aplicação**: K₅ (grafo completo com 5 vértices) tem E = 10 e V = 5, mas 3·5 - 6 = 9 &lt; 10. Logo, **K₅ não é planar**.

**Corolário para grafos bipartidos planares**: E ≤ 2V - 4 (pois faces têm pelo menos 4 arestas).

K₃,₃ (bipartido completo 3×3) tem E = 9 e V = 6, mas 2·6 - 4 = 8 &lt; 9. Logo, **K₃,₃ não é planar**.

**Teorema de Kuratowski**: Um grafo é planar se e somente se não contém uma subdivisão de K₅ ou K₃,₃.

**Aplicação em CS**: Planaridade é relevante em:
- **Layout de circuitos** (PCB design): circuitos planares são mais simples de fabricar
- **Visualização de grafos**: grafos planares podem ser desenhados sem cruzamentos
- **Algoritmos mais eficientes**: muitos problemas NP-hard em grafos gerais são polinomiais em grafos planares (ex: planar separator theorem)

### 6.8 Coloração de Grafos

Uma **k-coloração** de G é uma função `c: V → {1, ..., k}` tal que vértices adjacentes recebem cores diferentes: se &#123;u, v&#125; ∈ E, então c(u) ≠ c(v).

O **número cromático** χ(G) é o menor k para o qual G tem uma k-coloração.

| Classe de Grafos | χ(G) |
|---|---|
| Grafo vazio (sem arestas) | 1 |
| Árvore (com ≥ 1 aresta) | 2 |
| Ciclo par (C₂ₖ) | 2 |
| Ciclo ímpar (C₂ₖ₊₁) | 3 |
| Grafo bipartido | ≤ 2 |
| Grafo planar | ≤ 4 (Teorema das 4 Cores) |
| Grafo completo Kₙ | n |

**Teorema das 4 Cores**: Todo grafo planar pode ser colorido com no máximo 4 cores. A prova (Appel & Haken, 1976) foi a primeira a usar verificação computacional extensiva — analisou 1.936 configurações redutíveis via computador.

**Aplicação: Register Allocation em Compiladores**:

O problema de register allocation é: dado um programa com variáveis, atribuir cada variável a um registrador (finito) da CPU tal que variáveis "vivas" simultaneamente recebam registradores diferentes.

1. Construa o **grafo de interferência**: vértices = variáveis, aresta entre u e v se são "vivas" ao mesmo tempo.
2. Encontre uma k-coloração (onde k = número de registradores disponíveis).
3. Se χ(G) ≤ k, todas as variáveis cabem nos registradores. Senão, algumas precisam ser "spilled" para a memória.

O problema geral de k-coloração é NP-completo, mas heurísticas gulosas (como o algoritmo de Chaitin) funcionam muito bem na prática para grafos de interferência típicos.

```typescript
/**
 * Coloração gulosa de grafos.
 * Não garante o número mínimo de cores (χ(G)),
 * mas usa no máximo Δ(G) + 1 cores, onde Δ é o grau máximo.
 *
 * O Teorema de Brooks melhora isso: se G não é um clique nem um ciclo ímpar,
 * então χ(G) ≤ Δ(G).
 */
function greedyColoring(
  adjList: Map<number, number[]>
): Map<number, number> {
  const color = new Map<number, number>();

  for (const node of adjList.keys()) {
    // Cores usadas pelos vizinhos
    const usedColors = new Set<number>();
    for (const neighbor of adjList.get(node) ?? []) {
      if (color.has(neighbor)) {
        usedColors.add(color.get(neighbor)!);
      }
    }

    // Encontra a menor cor disponível
    let c = 0;
    while (usedColors.has(c)) c++;
    color.set(node, c);
  }

  return color;
}

// Grafo: ciclo de 5 vértices (C5) — χ = 3
const c5 = new Map([
  [0, [1, 4]],
  [1, [0, 2]],
  [2, [1, 3]],
  [3, [2, 4]],
  [4, [3, 0]],
]);

const coloring = greedyColoring(c5);
console.log(coloring);
// Map { 0 => 0, 1 => 1, 2 => 0, 3 => 1, 4 => 2 }
// 3 cores usadas — ótimo para C5

const numColors = new Set(coloring.values()).size;
console.log(`Cores usadas: ${numColors}`); // 3
```

### 6.9 Grafos Eulerianos e Hamiltonianos

| Propriedade | Euleriano | Hamiltoniano |
|---|---|---|
| Definição | Contém um circuito que visita **cada aresta** exatamente uma vez | Contém um ciclo que visita **cada vértice** exatamente uma vez |
| Condição (necessária e suficiente para Euler) | Grafo conexo e **todo vértice tem grau par** | Não existe condição simples (NP-completo!) |
| Complexidade | Verificação: O(V+E); Construção: O(E) | Verificação: NP-completo |
| Exemplo | Pontes de Konigsberg (Euler, 1736) | Problema do Caixeiro Viajante (TSP) |

**Teorema de Euler (1736)**: Um grafo conexo tem um **circuito euleriano** se e somente se todo vértice tem grau par. Tem um **caminho euleriano** (que não precisa fechar) se e somente se tem exatamente 0 ou 2 vértices de grau ímpar.

**Prova (necessidade para circuito)**: Quando o circuito passa por um vértice, usa uma aresta para entrar e uma para sair — consome 2 arestas. Como todas as arestas devem ser usadas, cada vértice tem grau par.

**Aplicação em CS**: O problema de **Chinese Postman** (rota mínima que percorre todas as arestas pelo menos uma vez) — usado em planejamento de rotas de entrega, inspeção de redes e garbage collection em grafos.

### 6.10 Resumo de Propriedades Fundamentais

| Classe | V | E | Conectividade | Planar | χ(G) |
|---|---|---|---|---|---|
| Árvore | n | n-1 | Conexo, acíclico | Sim | 2 (se n ≥ 2) |
| Ciclo Cₙ | n | n | 2-conexo | Sim | 2 (n par), 3 (n ímpar) |
| Completo Kₙ | n | n(n-1)/2 | (n-1)-conexo | n ≤ 4 | n |
| Bipartido completo Kₘ,ₙ | m+n | m·n | Conexo | m ≤ 2 ou n ≤ 2 | 2 |
| Hipercubo Qₙ | 2ⁿ | n·2ⁿ⁻¹ | n-conexo | n ≤ 3 | 2 |
| Petersen | 10 | 15 | 3-conexo | Não | 3 |

> **Nota**: Esta seção fornece a base teórica formal de grafos. A implementação prática — representações em memória (lista de adjacência, matriz), algoritmos de busca (BFS, DFS), caminhos mínimos (Dijkstra, Bellman-Ford), árvores geradoras mínimas (Kruskal, Prim) e fluxo em redes — é coberta na lição dedicada a **Graphs**.

---

## 7. Síntese — Conexões Entre os Tópicos

Os seis pilares desta lição se conectam profundamente:

1. **Indução** prova a **correção** de algoritmos que operam sobre estruturas discretas. Merge sort, binary search, árvores, grafos — todos são verificados formalmente por indução (fraca, forte ou estrutural).

2. **Combinatória** quantifica o **espaço de possibilidades** que um algoritmo precisa explorar. Quando C(n, r) ou n! definem o tamanho do espaço de busca, sabemos imediatamente a complexidade de soluções brute-force e a necessidade de técnicas como programação dinâmica ou greedy.

3. **Probabilidade** analisa o **comportamento esperado** de algoritmos randomizados. A linearidade da esperança prova que o quicksort randomizado é O(n log n) em média. O Birthday Paradox quantifica colisões em hash tables.

4. **Relações** fornecem a **estrutura algébrica** de partições (Union-Find), dependências (ordenação topológica) e equivalências (classes de tipos, componentes conexos).

5. **O Pigeonhole Principle** prova **inevitabilidades** — colisões em hash tables, repetições em strings, loops em roteamento. Sempre que há mais objetos que categorias, alguma categoria é "sobrecarregada".

6. **Teoria de grafos** unifica tudo: grafos são a linguagem matemática para redes, dependências, estados, relações e fluxos. Os conceitos formais desta seção fundamentam os algoritmos implementados na lição de Graphs.

Dominar esses fundamentos transforma a intuição em **certeza matemática**. Em vez de "achar" que um algoritmo está correto, você **prova**. Em vez de "estimar" o número de operações, você **calcula**. Essa precisão é o que separa engenharia de software de programação ad hoc.
