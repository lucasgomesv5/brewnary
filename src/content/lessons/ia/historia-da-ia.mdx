---
title: "Historia da IA"
description: "Da maquina de Turing aos LLMs modernos: os marcos, as crises e as revolucoes que moldaram a Inteligencia Artificial"
track: "ia"
order: 2
section: "Fundamentos"
priority: "medium"
tags: ["historia", "ia", "turing", "deep-learning"]
prerequisites: []
keyTakeaways:
  - "A IA nasceu formalmente em 1956 na conferencia de Dartmouth, mas suas raizes remontam ao Teste de Turing (1950) e a logica formal de decadas anteriores"
  - "Os AI Winters (1974-1980 e 1987-1993) foram periodos de reducao drastica de financiamento causados por expectativas irreais e limitacoes tecnicas -- entender esses ciclos previne o hype cego"
  - "O momento de inflexao moderno foi o ImageNet 2012 (AlexNet) para visao computacional e o paper Attention Is All You Need (2017) para NLP -- Transformers sao a base de todos os LLMs atuais"
---

# Historia da IA

## 1. Fundacoes (1940s-1955)

A historia da IA comeca antes do termo existir, com trabalhos fundamentais em logica, computacao e neurociencia.

### Alan Turing e a Maquina Universal (1936)

Em 1936, Alan Turing publicou *On Computable Numbers*, definindo a **Maquina de Turing** -- um modelo teorico de computacao que formalizou o que significa "computar". Qualquer funcao computavel pode ser executada por uma Maquina de Turing, e todo computador moderno e equivalente a uma (Tese de Church-Turing).

### O Neuronio Artificial de McCulloch-Pitts (1943)

Warren McCulloch e Walter Pitts propuseram o primeiro modelo matematico de um neuronio artificial. O modelo era simples: entradas binarias, pesos, threshold e saida binaria. Demonstrou que redes de neuronios artificiais poderiam, em teoria, computar qualquer funcao logica.

```
Modelo McCulloch-Pitts (1943):

  x1 ──(w1)──┐
              │
  x2 ──(w2)──┼──→ [ sum >= threshold? ] ──→ y (0 ou 1)
              │
  x3 ──(w3)──┘

  y = 1  se  (w1*x1 + w2*x2 + w3*x3) >= threshold
  y = 0  caso contrario
```

### O Teste de Turing (1950)

No paper *Computing Machinery and Intelligence*, Turing propos o **Imitation Game** (hoje conhecido como Teste de Turing): um avaliador humano conversa via texto com uma maquina e um humano; se o avaliador nao consegue distinguir a maquina do humano de forma consistente, a maquina demonstra "inteligencia".

O Teste de Turing permanece relevante como referencia conceitual, embora tenha limitacoes conhecidas: um sistema pode parecer inteligente sem entender nada (argumento do Chinese Room de John Searle, 1980), e inteligencia nao requer necessariamente imitacao humana.

---

## 2. O Nascimento Formal da IA (1956-1969)

### A Conferencia de Dartmouth (1956)

No verao de 1956, John McCarthy, Marvin Minsky, Nathaniel Rochester e Claude Shannon organizaram um workshop de 6 semanas no Dartmouth College. A proposta continha a frase historica:

> *"Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it."*

Este evento e considerado o **marco zero** da IA como campo de pesquisa. O proprio termo "Artificial Intelligence" foi cunhado por McCarthy para o workshop.

### Primeiros Programas de IA

```
LINHA DO TEMPO: PRIMEIROS MARCOS
────────────────────────────────────────────────────────

1956  Logic Theorist (Newell & Simon)
      Primeiro programa capaz de provar teoremas matematicos.
      Provou 38 dos 52 teoremas do Principia Mathematica.

1957  Perceptron (Frank Rosenblatt)
      Primeiro algoritmo de aprendizado para redes neurais.
      Classificador binario linear com regra de aprendizado.

1958  LISP (John McCarthy)
      Linguagem de programacao que se tornou o padrao da IA
      por decadas. Garbage collection, recursao, listas.

1959  GPS - General Problem Solver (Newell & Simon)
      Tentativa de criar um resolvedor universal de problemas
      usando means-ends analysis.

1964  ELIZA (Joseph Weizenbaum, MIT)
      Chatbot que imitava um psicoterapeuta usando pattern
      matching simples. Pessoas atribuiam "compreensao" ao
      programa -- o primeiro caso documentado do "efeito ELIZA".

1966  SHRDLU (Terry Winograd)
      Sistema de NLP que manipulava blocos em um mundo virtual.
      Entendia comandos em ingles natural dentro de um dominio
      restrito: "Pick up the red block and put it on the blue one."
```

### Otimismo Excessivo

Os pioneiros da IA fizeram previsoes extremamente otimistas:

- **Herbert Simon (1957)**: "Dentro de 10 anos, um computador sera campeao mundial de xadrez" (aconteceu em 1997, 40 anos depois)
- **Marvin Minsky (1967)**: "Dentro de uma geracao, o problema de criar IA sera substancialmente resolvido" (ainda nao foi)

Esse otimismo plantou a semente para a frustracao que viria a seguir.

---

## 3. O Primeiro AI Winter (1974-1980)

### Causas

O **relatorio Lighthill** (1973), encomendado pelo governo britanico, concluiu que a IA nao havia cumprido suas promessas grandiosas. Os principais problemas tecnicos:

- **Explosao combinatoria**: problemas do mundo real tinham espacos de busca exponencialmente maiores do que os exemplos demonstrados
- **Limitacoes do Perceptron**: o livro *Perceptrons* (Minsky & Papert, 1969) provou matematicamente que perceptrons de camada unica nao podiam resolver problemas nao-linearmente separaveis (como XOR). Embora redes multicamada resolvessem isso, o livro desencorajou pesquisa em redes neurais por mais de uma decada
- **Falta de dados e computacao**: os algoritmos existiam, mas nao havia dados nem hardware para torna-los praticos

### Consequencias

- Financiamento governamental (DARPA, governo britanico) cortado drasticamente
- Pesquisadores migraram para outros campos
- O termo "IA" se tornou toxico em propostas de financiamento -- pesquisadores rebatizaram seus trabalhos como "sistemas inteligentes", "informatica" ou "ciencia cognitiva"

---

## 4. Sistemas Especialistas e o Segundo Boom (1980-1987)

### A Era dos Expert Systems

Sistemas especialistas (expert systems) codificavam o conhecimento de especialistas humanos em regras if/then e as aplicavam para tomar decisoes em dominios especificos.

```
Exemplo: MYCIN (Stanford, 1976) -- diagnostico de infeccoes bacterianas

REGRA 001:
  SE   a cultura do organismo e gram-positiva
  E    a morfologia do organismo e coco
  E    a formacao de crescimento e em cadeia
  ENTAO existe 70% de probabilidade de que o organismo
        seja Streptococcus

Base de conhecimento: ~600 regras
Resultado: diagnostico correto em 65% dos casos
           (comparavel a especialistas humanos da epoca)
```

**Sistemas notaveis:**
- **MYCIN** (1976): diagnostico medico
- **DENDRAL** (1965-1981): analise de estruturas quimicas
- **R1/XCON** (1980): configuracao de computadores DEC -- economizou ~$40M/ano
- **Cyc** (1984-presente): tentativa de codificar todo o senso comum humano em logica formal

### O Hardware de IA: Maquinas LISP

Empresas como Symbolics, LMI e Texas Instruments produziram computadores dedicados a executar LISP. O mercado atingiu bilhoes de dolares. O Japao lancou o projeto **Fifth Generation Computer Systems** (1982) com o objetivo de criar computadores baseados em logica (Prolog).

---

## 5. O Segundo AI Winter (1987-1993)

### Causas

- **Sistemas especialistas eram frageis**: qualquer situacao fora das regras codificadas causava falha. Nao aprendiam nem se adaptavam
- **Custos proibitivos**: manter uma base de conhecimento com milhares de regras exigia equipes de "engenheiros de conhecimento"
- **O colapso do mercado LISP**: computadores pessoais (PCs) ficaram poderosos o suficiente para rodar as mesmas aplicacoes. Empresas de maquinas LISP faliram
- **O projeto japones falhou**: o Fifth Generation nao produziu resultados significativos

### Consequencias

Novamente, financiamento cortado e pessimismo generalizado. Pesquisadores de IA passaram a usar termos como "machine learning", "data mining" e "analytics" para evitar o estigma.

---

## 6. O Renascimento do Machine Learning (1990s-2011)

### Marcos Fundamentais

```
LINHA DO TEMPO: RENASCIMENTO
────────────────────────────────────────────────────────

1986  Backpropagation popularizado (Rumelhart, Hinton, Williams)
      O paper "Learning representations by back-propagating errors"
      mostrou como treinar redes neurais multicamada.
      Tecnicamente inventado antes, mas este paper o tornou pratico.

1989  LeNet-5 (Yann LeCun)
      CNN que reconhecia digitos escritos a mao (MNIST).
      Usado pelo servico postal dos EUA para ler CEPs.

1997  Deep Blue vence Kasparov
      IBM Deep Blue derrotou o campeao mundial de xadrez.
      Usava busca bruta + avaliacao heuristica, nao ML.
      Mas demonstrou que maquinas podiam vencer humanos.

1997  LSTM (Hochreiter & Schmidhuber)
      Long Short-Term Memory resolve o problema do vanishing
      gradient em redes recorrentes. Essencial para NLP
      por mais de uma decada.

2006  "Deep Learning" cunhado (Geoffrey Hinton)
      Paper sobre Deep Belief Networks renovou interesse em
      redes neurais profundas. Pre-treinamento greedy layer-wise.

2009  ImageNet (Fei-Fei Li)
      Dataset com 14 milhoes de imagens rotuladas em 20.000
      categorias. Base do benchmark que impulsionou deep learning.

2011  Watson vence Jeopardy! (IBM)
      Sistema de NLP derrotou campeoes humanos no quiz show.
      Combinou busca, NLP e knowledge graphs.
```

### Por que ML voltou a funcionar?

Tres fatores convergiram:

1. **Dados**: a internet gerou volumes massivos de dados rotulados (imagens, texto, transacoes)
2. **Computacao**: GPUs (originalmente para jogos) se mostraram ideais para algebra linear massivamente paralela
3. **Algoritmos**: backpropagation + novas arquiteturas (LSTM, CNNs profundas) tornaram o treinamento de redes profundas viavel

---

## 7. A Revolucao do Deep Learning (2012-2016)

### AlexNet e o ImageNet 2012

O marco que inaugurou a era moderna do deep learning. Alex Krizhevsky, Ilya Sutskever e Geoffrey Hinton treinaram uma CNN profunda (**AlexNet**) no ImageNet e reduziram o erro de classificacao de 26% para **16.4%** -- uma melhoria sem precedentes.

```
ImageNet Large Scale Visual Recognition Challenge (ILSVRC)
Top-5 Error Rate:

2010: 28.2%  (metodos tradicionais -- SIFT + SVM)
2011: 25.8%  (metodos tradicionais)
2012: 16.4%  ← AlexNet (CNN profunda com GPU) -- RUPTURA
2013: 11.7%  (ZFNet)
2014:  6.7%  (GoogLeNet/Inception, VGGNet)
2015:  3.6%  (ResNet -- 152 camadas!) -- ABAIXO DO NIVEL HUMANO (~5%)
```

**Por que AlexNet foi revolucionario:**
- Usou GPUs (2x NVIDIA GTX 580) para treinamento -- 6x mais rapido que CPU
- ReLU em vez de sigmoid/tanh (treinamento mais rapido, menos vanishing gradient)
- Dropout para regularizacao
- Data augmentation (flip, crop, color jitter)

### Marcos pos-AlexNet

```
2014  GANs - Generative Adversarial Networks (Ian Goodfellow)
      Duas redes neurais competindo: gerador vs discriminador.
      Base para geracao de imagens, deepfakes e mais.

2014  Seq2Seq + Attention (Bahdanau et al.)
      Mecanismo de atencao para traducao automatica.
      A semente que germinou nos Transformers.

2015  ResNet (Kaiming He et al.)
      Residual connections permitiram treinar redes com 152+
      camadas. Superou nivel humano no ImageNet.

2016  AlphaGo vence Lee Sedol (DeepMind)
      Sistema combinou deep learning + MCTS + reinforcement
      learning para dominar Go -- considerado impossivel para
      maquinas pela complexidade combinatoria (10^170 posicoes).
```

---

## 8. A Era dos Transformers (2017-2022)

### Attention Is All You Need (2017)

O paper de Vaswani et al. (Google Brain) introduziu a arquitetura **Transformer**, eliminando recorrencia (RNN/LSTM) e convolucao em favor de **self-attention** pura. O impacto foi sismico.

```
Evolucao das arquiteturas dominantes em NLP:

RNNs (1990s)      →  Processamento sequencial, lento,
                      vanishing gradient
LSTMs (1997)      →  Resolve vanishing gradient, mas ainda
                      sequencial (nao paralelizavel)
Attention (2014)  →  Permite "olhar" para toda a sequencia,
                      mas ainda dentro de RNNs
Transformer (2017) → PURO attention, totalmente paralelizavel,
                      escala para sequencias muito longas
```

### A Explosao dos Modelos Pre-Treinados

```
2018  BERT (Google)
      Bidirectional Encoder -- revolucionou NLP.
      Pre-treinamento: masked language model + next sentence prediction.
      Fine-tuning para tarefas downstream: QA, NER, classificacao.
      Resultado: SOTA em 11 benchmarks de NLP simultaneamente.

2018  GPT-1 (OpenAI)
      117M parametros. Decoder-only Transformer.
      Pre-treinamento: next token prediction (autoregressive).
      Demonstrou que pre-treino + fine-tuning era poderoso.

2019  GPT-2 (OpenAI)
      1.5B parametros. "Too dangerous to release" (controverso).
      Zero-shot em varias tarefas sem fine-tuning.

2020  GPT-3 (OpenAI)
      175B parametros. In-context learning: resolve tarefas
      apenas com exemplos no prompt (few-shot), sem fine-tuning.
      Mudou o paradigma: de "treinar para cada tarefa" para
      "descrever a tarefa no prompt".

2020  AlphaFold 2 (DeepMind)
      Resolveu o protein folding problem -- previsao da estrutura
      3D de proteinas a partir da sequencia de aminoacidos.
      Precisao comparavel a cristalografia experimental.
```

---

## 9. A Era dos LLMs e IA Generativa (2022-Presente)

### O Momento ChatGPT

Em novembro de 2022, a OpenAI lancou o **ChatGPT** (baseado no GPT-3.5), que atingiu 100 milhoes de usuarios em 2 meses -- o crescimento mais rapido de qualquer aplicacao na historia. Isso desencadeou uma corrida por LLMs entre as maiores empresas de tecnologia.

```
LINHA DO TEMPO: ERA DOS LLMs
────────────────────────────────────────────────────────

2022 Mar  InstructGPT (OpenAI)
          Introduziu RLHF para alinhar LLMs com instrucoes humanas.

2022 Jun  Chinchilla (DeepMind)
          Scaling laws: para budget fixo de compute, e melhor
          treinar um modelo MENOR com MAIS dados do que um
          modelo maior com menos dados.

2022 Nov  ChatGPT (OpenAI)
          GPT-3.5 + RLHF + interface de chat.
          100M usuarios em 2 meses.

2023 Mar  GPT-4 (OpenAI)
          Multimodal (texto + imagem). Salto de qualidade
          significativo. Passou no exame da OAB americana.

2023 Mar  Claude (Anthropic)
          Foco em seguranca e alinhamento (Constitutional AI).

2023 Jul  Llama 2 (Meta)
          Modelos open-weight. Democratizou acesso a LLMs
          de alta qualidade para pesquisa e producao.

2023 Dec  Gemini (Google DeepMind)
          Multimodal nativo (texto, imagem, audio, video, codigo).

2024      Escalada de modelos: Claude 3 Opus, GPT-4 Turbo,
          Llama 3, Mistral, modelos open-source cada vez mais
          capazes. Foco em agentes, tool use e RAG.

2025      Claude 4, o1/o3 (OpenAI reasoning models), DeepSeek,
          Qwen. Foco em raciocinio, agentes autonomos e
          integracao com ferramentas.
```

### Imagens, Audio e Video

A IA generativa expandiu alem de texto:

- **DALL-E 2/3** (OpenAI): geracao de imagens a partir de texto
- **Midjourney**: geracao de arte a partir de prompts
- **Stable Diffusion** (Stability AI): modelo open-source de geracao de imagens (diffusion models)
- **Whisper** (OpenAI): transcricao de audio state-of-the-art
- **Sora** (OpenAI): geracao de video a partir de texto

---

## 10. Licoes dos Ciclos de IA

A historia da IA revela padroes que se repetem:

```
Ciclo tipico:
  Avancos tecnicos → Hype excessivo → Promessas nao cumpridas
  → Corte de financiamento → Periodo de "inverno"
  → Avancos incrementais silenciosos → Novo breakthrough → Repete

     Expectativa
         │    ╱╲               ╱╲
         │   ╱  ╲             ╱  ╲        ╱ ...
         │  ╱    ╲     ╱╲   ╱    ╲      ╱
         │ ╱      ╲   ╱  ╲ ╱      ╲    ╱
         │╱        ╲ ╱    ╲╱        ╲  ╱
         └──────────╳──────────────────╳──→ Tempo
                 Winter 1          Winter 2
         1956   1974  1980  1987  1993     2012→
```

**Licoes fundamentais:**

1. **A tecnologia precede a utilidade em decadas**: backpropagation foi proposto nos anos 1960s, popularizado em 1986 e so se tornou pratico com GPUs nos anos 2010s
2. **Dados + Compute > Algoritmos sofisticados**: os algoritmos de deep learning existiam ha decadas; o que mudou foram os dados (internet) e o hardware (GPUs)
3. **Scaling laws sao reais**: mais parametros + mais dados + mais compute = melhor desempenho (dentro de limites e com retornos decrescentes)
4. **Ciclos de hype sao perigosos**: expectativas irreais levam a desilucao e cortes de financiamento que atrasam progresso real
5. **A IA como campo e resiliente**: apesar dos winters, pesquisadores continuaram trabalhando e cada retomada foi mais poderosa que a anterior

---

## Resumo Cronologico

```
1943  Neuronio McCulloch-Pitts
1950  Teste de Turing
1956  Conferencia de Dartmouth (nasce a "IA")
1957  Perceptron (Rosenblatt)
1969  Livro Perceptrons (Minsky & Papert) -- desencoraja redes neurais
1974  Primeiro AI Winter comeca
1980  Sistemas especialistas resurgem
1986  Backpropagation popularizado (Hinton et al.)
1987  Segundo AI Winter comeca
1997  Deep Blue, LSTM
2006  Deep Learning (Hinton)
2012  AlexNet -- RUPTURA no ImageNet
2014  GANs, Seq2Seq + Attention
2015  ResNet supera nivel humano
2016  AlphaGo
2017  Transformer ("Attention Is All You Need")
2018  BERT, GPT-1
2020  GPT-3, AlphaFold 2
2022  ChatGPT, Stable Diffusion
2023  GPT-4, Claude, Llama 2
2024  Claude 3, Gemini, agentes de IA
2025  Modelos de raciocinio, agentes autonomos
```

A IA nao nasceu pronta -- ela foi construida ao longo de 80 anos de avancos incrementais, fracassos espetaculares e breakthroughs inesperados. Entender essa historia fornece perspectiva critica para avaliar o presente e antecipar o futuro.
